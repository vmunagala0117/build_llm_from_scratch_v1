{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.5.1\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'My': 0, 'are': 2, 'big.': 3, 'feet': 4, 'my': 5, 'shoes': 6, 'small': 7}\n"
     ]
    }
   ],
   "source": [
    "sentence = 'My shoes are small, my feet are big.'\n",
    "\n",
    "dc = {s:i for i,s in enumerate(sorted(sentence.replace(',', '').split()))}\n",
    "print(dc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Assign index to each word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 6, 2, 7, 5, 4, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "sentence_int = torch.tensor([dc[s] for s in sentence.replace(',', '').split()])\n",
    "print(sentence_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, using the integer-vector representation of the input sentence, we can use an embedding layer to encode the inputs into a real-vector embedding. Here, we will use a 2-dimensional embedding such that each input word is represented by a 2-dimensional vector. Since the sentence consists of 8 words, this will result in a 8 X 2 dimensional embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3374, -0.1778],\n",
      "        [ 0.1794,  1.8951],\n",
      "        [ 0.3486,  0.6603],\n",
      "        [ 0.4954,  0.2692],\n",
      "        [ 0.6984, -1.4097],\n",
      "        [ 0.7671, -1.1925],\n",
      "        [ 0.3486,  0.6603],\n",
      "        [-0.2196, -0.3792]])\n",
      "torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "embed = torch.nn.Embedding(8, 2)\n",
    "embedded_sentence = embed(sentence_int).detach()\n",
    "\n",
    "print(embedded_sentence)\n",
    "print(embedded_sentence.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s discuss the widely utilized self-attention mechanism known as the scaled dot-product attention, which is integrated into the transformer architecture.\n",
    "\n",
    "Self-attention utilizes three weight matrices, referred to as $W_q$, $W_k$ and $W_v$ which are adjusted as model parameters during training. These matrices serve to project the inputs into query, key, and value components of the sequence, respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are computing the dot-product between the query and key vectors, these two vectors have to contain the same number of elements, However, the number of elements in the value vector $v^{(i)}$, which determines the size of the resulting context vector, is arbitrary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be extending the dimensions for query and keys to 3 and values to 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "d = embedded_sentence.shape[1]\n",
    "\n",
    "d_q, d_k, d_v = 3, 3, 4\n",
    "\n",
    "W_query = torch.nn.Parameter(torch.rand(d_q, d))\n",
    "W_key = torch.nn.Parameter(torch.rand(d_k, d))\n",
    "W_value = torch.nn.Parameter(torch.rand(d_v, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_sentence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.2961, 0.5166],\n",
      "        [0.2517, 0.6886],\n",
      "        [0.0740, 0.8665]], requires_grad=True)\n",
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "print(W_query)\n",
    "print(W_query.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.1366, 0.1025],\n",
      "        [0.1841, 0.7264],\n",
      "        [0.3153, 0.6871]], requires_grad=True)\n",
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "print(W_key)\n",
    "print(W_key.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.0756, 0.1966],\n",
      "        [0.3164, 0.4017],\n",
      "        [0.1186, 0.8274],\n",
      "        [0.3821, 0.6605]], requires_grad=True)\n",
      "torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "print(W_value)\n",
    "print(W_value.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Attention Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s suppose we are interested in computing the attention-vector for the second input element – the second input element acts as the query here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_2: tensor([0.1794, 1.8951]) \n",
      " x_2.shape: torch.Size([2])\n",
      "W_query: Parameter containing:\n",
      "tensor([[0.2961, 0.5166],\n",
      "        [0.2517, 0.6886],\n",
      "        [0.0740, 0.8665]], requires_grad=True) \n",
      " ... query_2: tensor([1.0321, 1.3501, 1.6555], grad_fn=<MvBackward0>) \n",
      "\n",
      "W_key: Parameter containing:\n",
      "tensor([[0.1366, 0.1025],\n",
      "        [0.1841, 0.7264],\n",
      "        [0.3153, 0.6871]], requires_grad=True) \n",
      " ... key_2: tensor([0.2187, 1.4097, 1.3587], grad_fn=<MvBackward0>) \n",
      "\n",
      "W_value: Parameter containing:\n",
      "tensor([[0.0756, 0.1966],\n",
      "        [0.3164, 0.4017],\n",
      "        [0.1186, 0.8274],\n",
      "        [0.3821, 0.6605]], requires_grad=True) \n",
      " ... value_2: tensor([0.3862, 0.8181, 1.5893, 1.3203], grad_fn=<MvBackward0>) \n",
      "\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "x_2 = embedded_sentence[1]\n",
    "query_2 = W_query.matmul(x_2)\n",
    "key_2 = W_key.matmul(x_2)\n",
    "value_2 = W_value.matmul(x_2)\n",
    "\n",
    "print(f\"x_2: {x_2} \\n x_2.shape: {x_2.shape}\")\n",
    "print(f\"W_query: {W_query} \\n ... query_2: {query_2} \\n\")\n",
    "print(f\"W_key: {W_key} \\n ... key_2: {key_2} \\n\")\n",
    "print(f\"W_value: {W_value} \\n ... value_2: {value_2} \\n\")\n",
    "print(query_2.shape)\n",
    "print(key_2.shape)\n",
    "print(value_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0321289999999999"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the matmul \n",
    "(0.1794*0.2961) + (1.8951*0.5166)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- These three matrices are used to project the embedded input tokens, $x^{(i)}$, into query, key, and value vectors via matrix multiplication:\n",
    "\n",
    "  - Query vector: $q^{(i)} = W_q \\,x^{(i)}$\n",
    "  - Key vector: $k^{(i)} = W_k \\,x^{(i)}$\n",
    "  - Value vector: $v^{(i)} = W_v \\,x^{(i)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([8, 3])\n",
      "values.shape: torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "keys = W_key.matmul(embedded_sentence.T).T\n",
    "values = W_value.matmul(embedded_sentence.T).T\n",
    "\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0279, -0.0671, -0.0158],\n",
       "        [ 0.2187,  1.4097,  1.3587],\n",
       "        [ 0.1153,  0.5439,  0.5636],\n",
       "        [ 0.0953,  0.2867,  0.3412],\n",
       "        [-0.0491, -0.8956, -0.7485],\n",
       "        [-0.0174, -0.7251, -0.5775],\n",
       "        [ 0.1153,  0.5439,  0.5636],\n",
       "        [-0.0689, -0.3159, -0.3298]], grad_fn=<PermuteBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then generalize this to compute th remaining key, and value elements for all inputs as well, since we will need them in the next step when we compute the unnormalized attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0094,  0.0353, -0.1071,  0.0115],\n",
       "        [ 0.3862,  0.8181,  1.5893,  1.3203],\n",
       "        [ 0.1562,  0.3756,  0.5877,  0.5693],\n",
       "        [ 0.0904,  0.2649,  0.2815,  0.3671],\n",
       "        [-0.2244, -0.3454, -1.0836, -0.6643],\n",
       "        [-0.1765, -0.2364, -0.8957, -0.4945],\n",
       "        [ 0.1562,  0.3756,  0.5877,  0.5693],\n",
       "        [-0.0912, -0.2218, -0.3398, -0.3344]], grad_fn=<PermuteBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3374, -0.1778],\n",
       "        [ 0.1794,  1.8951],\n",
       "        [ 0.3486,  0.6603],\n",
       "        [ 0.4954,  0.2692],\n",
       "        [ 0.6984, -1.4097],\n",
       "        [ 0.7671, -1.1925],\n",
       "        [ 0.3486,  0.6603],\n",
       "        [-0.2196, -0.3792]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3374,  0.1794,  0.3486,  0.4954,  0.6984,  0.7671,  0.3486, -0.2196],\n",
       "        [-0.1778,  1.8951,  0.6603,  0.2692, -1.4097, -1.1925,  0.6603, -0.3792]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_sentence.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the unnormalized attention weights  $\\omega$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we compute $\\omega_{ij}$ as the dot product between the query and key sequences,\n",
    "$\\omega_{ij}$ = q^{(i)}k^{(j)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we can compute the unnormalized attention weight for the query and 5th input element (corresponding to index position 4) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0321, 1.3501, 1.6555], grad_fn=<MvBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0279, -0.0671, -0.0158],\n",
       "        [ 0.2187,  1.4097,  1.3587],\n",
       "        [ 0.1153,  0.5439,  0.5636],\n",
       "        [ 0.0953,  0.2867,  0.3412],\n",
       "        [-0.0491, -0.8956, -0.7485],\n",
       "        [-0.0174, -0.7251, -0.5775],\n",
       "        [ 0.1153,  0.5439,  0.5636],\n",
       "        [-0.0689, -0.3159, -0.3298]], grad_fn=<PermuteBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.49896742\n"
     ]
    }
   ],
   "source": [
    "# query_2.dot(keys[4])\n",
    "test_omega_24 = -0.0491 * 1.0321 + -0.8956 * 1.3501 +  -0.7485 * 1.6555\n",
    "print(test_omega_24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.08795302000000002\n"
     ]
    }
   ],
   "source": [
    "#omaega_20 = [0.0279,-0.0671,-0.0158] * [1.0321, 1.3501, 1.6555]\n",
    "\n",
    "omega_20 = 0.0279 * 1.0321 + -0.0671 * 1.3501 + -0.0158 * 1.6555\n",
    "print(omega_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2.4988, grad_fn=<DotBackward0>)\n"
     ]
    }
   ],
   "source": [
    "omega_24 = query_2.dot(keys[4])\n",
    "print(omega_24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0879,  4.3783,  1.7863,  1.0502, -2.4988, -1.9530,  1.7863, -1.0434],\n",
      "       grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "omega_2 = query_2.matmul(keys.T)\n",
    "print(omega_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the Attention Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0432, 0.5687, 0.1273, 0.0832, 0.0107, 0.0147, 0.1273, 0.0249],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "attention_weights_2 = F.softmax(omega_2 / d_k**0.5, dim=0)\n",
    "print(attention_weights_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0094,  0.0353, -0.1071,  0.0115],\n",
       "        [ 0.3862,  0.8181,  1.5893,  1.3203],\n",
       "        [ 0.1562,  0.3756,  0.5877,  0.5693],\n",
       "        [ 0.0904,  0.2649,  0.2815,  0.3671],\n",
       "        [-0.2244, -0.3454, -1.0836, -0.6643],\n",
       "        [-0.1765, -0.2364, -0.8957, -0.4945],\n",
       "        [ 0.1562,  0.3756,  0.5877,  0.5693],\n",
       "        [-0.0912, -0.2218, -0.3398, -0.3344]], grad_fn=<PermuteBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8])\n",
      "torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "print(attention_weights_2.shape)\n",
    "print(values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4])\n",
      "tensor([0.2593, 0.5718, 1.0390, 0.9041], grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "context_vector_2 = attention_weights_2.matmul(values)\n",
    "\n",
    "print(context_vector_2.shape)\n",
    "print(context_vector_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0094,  0.3862,  0.1562,  0.0904, -0.2244, -0.1765,  0.1562, -0.0912],\n",
      "        [ 0.0353,  0.8181,  0.3756,  0.2649, -0.3454, -0.2364,  0.3756, -0.2218],\n",
      "        [-0.1071,  1.5893,  0.5877,  0.2815, -1.0836, -0.8957,  0.5877, -0.3398],\n",
      "        [ 0.0115,  1.3203,  0.5693,  0.3671, -0.6643, -0.4945,  0.5693, -0.3344]],\n",
      "       grad_fn=<PermuteBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(values.T)\n",
    "(values.T).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [0.0432, 0.5687, 0.1273, 0.0832, 0.0107, 0.0147, 0.1273, 0.0249] * tensor([[-0.0094,  0.0353, -0.1071,  0.0115],\n",
    "#                                                                            [ 0.3862,  0.8181,  1.5893,  1.3203],\n",
    "#                                                                            [ 0.1562,  0.3756,  0.5877,  0.5693],\n",
    "#                                                                            [ 0.0904,  0.2649,  0.2815,  0.3671],\n",
    "#                                                                            [-0.2244, -0.3454, -1.0836, -0.6643],\n",
    "#                                                                            [-0.1765, -0.2364, -0.8957, -0.4945],\n",
    "#                                                                            [ 0.1562,  0.3756,  0.5877,  0.5693],\n",
    "#                                                                            [-0.0912, -0.2218, -0.3398, -0.3344]]\n",
    "\n",
    "context_vector_query2_1 = (0.0432 * -0.0094) + (0.5687 *  0.3862) +  (0.1273 * 0.1562) + (0.0832 * 0.0904) + (0.0107 * -0.2244) + (0.0147 * -0.1765) + (0.1273 * 0.1562) + (0.0249 * -0.0912)\n",
    "context_vector_query2_2 = (0.0432 * 0.0353) + (0.5687 *  0.8181) +  (0.1273 * 0.3756) + (0.0832 * 0.2649) + (0.0107 * -0.3454) + (0.0147 * -0.2364) + (0.1273 * 0.3756) + (0.0249 * -0.2218)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25924915, 0.57175219\n"
     ]
    }
   ],
   "source": [
    "print(f\"{context_vector_query2_1}, {context_vector_query2_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go with 3 heads now. \n",
    "- d_q, d_k, d_v = 3, 3, 4\n",
    "- d = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 3\n",
    "multihead_W_query = torch.nn.Parameter(torch.rand(h, d_q, d))\n",
    "multihead_W_key = torch.nn.Parameter(torch.rand(h, d_k, d))\n",
    "multihead_W_value = torch.nn.Parameter(torch.rand(h, d_v, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[0.8536, 0.5932],\n",
      "         [0.6367, 0.9826],\n",
      "         [0.2745, 0.6584]],\n",
      "\n",
      "        [[0.2775, 0.8573],\n",
      "         [0.8993, 0.0390],\n",
      "         [0.9268, 0.7388]],\n",
      "\n",
      "        [[0.7179, 0.7058],\n",
      "         [0.9156, 0.4340],\n",
      "         [0.0772, 0.3565]]], requires_grad=True)\n",
      "torch.Size([3, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "print(multihead_W_query)\n",
    "print(multihead_W_query.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(here, let’s keep the focus on the 3rd element corresponding to index position 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1794, 1.8951])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.2772, 1.9764, 1.2970],\n",
      "        [1.6745, 0.2353, 1.5663],\n",
      "        [1.4664, 0.9867, 0.6895]], grad_fn=<UnsafeViewBackward0>)\n",
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "multihead_query_2 = multihead_W_query.matmul(x_2)\n",
    "print(multihead_query_2)\n",
    "print(multihead_query_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys:\n",
      "tensor([[1.0367, 0.5123, 1.9268],\n",
      "        [1.0603, 1.1722, 1.6966],\n",
      "        [1.2750, 1.4245, 0.1450]], grad_fn=<UnsafeViewBackward0>)\n",
      "torch.Size([3, 3])\n",
      "Values:\n",
      "tensor([[1.7906, 0.2750, 1.8344, 0.3146],\n",
      "        [1.0396, 0.9122, 1.3936, 1.6952],\n",
      "        [0.2099, 1.0958, 0.1481, 1.7002]], grad_fn=<UnsafeViewBackward0>)\n",
      "torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "multihead_key_2 = multihead_W_key.matmul(x_2)\n",
    "multihead_value_2 = multihead_W_value.matmul(x_2)\n",
    "print(\"Keys:\")\n",
    "print(multihead_key_2)\n",
    "print(multihead_key_2.shape)\n",
    "print(\"Values:\")\n",
    "print(multihead_value_2)\n",
    "print(multihead_value_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, these key and value elements are specific to the query element. But, similar to earlier, we will also need the value and keys for the other sequence elements in order to compute the attention scores for the query. We can do this is by expanding the input sequence embeddings to size 3, i.e., the number of attention heads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3374, -0.1778],\n",
      "        [ 0.1794,  1.8951],\n",
      "        [ 0.3486,  0.6603],\n",
      "        [ 0.4954,  0.2692],\n",
      "        [ 0.6984, -1.4097],\n",
      "        [ 0.7671, -1.1925],\n",
      "        [ 0.3486,  0.6603],\n",
      "        [-0.2196, -0.3792]])\n",
      "torch.Size([8, 2])\n",
      "\n",
      "Transpose...\n",
      "tensor([[ 0.3374,  0.1794,  0.3486,  0.4954,  0.6984,  0.7671,  0.3486, -0.2196],\n",
      "        [-0.1778,  1.8951,  0.6603,  0.2692, -1.4097, -1.1925,  0.6603, -0.3792]])\n",
      "torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "print(embedded_sentence)\n",
    "print(embedded_sentence.shape)\n",
    "print(\"\\nTranspose...\")\n",
    "print(embedded_sentence.T)\n",
    "print(embedded_sentence.T.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have 3 attention heads, we will duplicate the input embeddings to size 3.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.3374,  0.1794,  0.3486,  0.4954,  0.6984,  0.7671,  0.3486,\n",
      "          -0.2196],\n",
      "         [-0.1778,  1.8951,  0.6603,  0.2692, -1.4097, -1.1925,  0.6603,\n",
      "          -0.3792]],\n",
      "\n",
      "        [[ 0.3374,  0.1794,  0.3486,  0.4954,  0.6984,  0.7671,  0.3486,\n",
      "          -0.2196],\n",
      "         [-0.1778,  1.8951,  0.6603,  0.2692, -1.4097, -1.1925,  0.6603,\n",
      "          -0.3792]],\n",
      "\n",
      "        [[ 0.3374,  0.1794,  0.3486,  0.4954,  0.6984,  0.7671,  0.3486,\n",
      "          -0.2196],\n",
      "         [-0.1778,  1.8951,  0.6603,  0.2692, -1.4097, -1.1925,  0.6603,\n",
      "          -0.3792]]])\n",
      "torch.Size([3, 2, 8])\n"
     ]
    }
   ],
   "source": [
    "stacked_inputs = embedded_sentence.T.repeat(3, 1, 1)\n",
    "print(stacked_inputs)\n",
    "print(stacked_inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can compute all the keys and values using via `torch.bmm()` (batch matrix multiplication):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
