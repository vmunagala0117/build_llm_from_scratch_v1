{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.5.1\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'My': 0, 'are': 2, 'big.': 3, 'feet': 4, 'my': 5, 'shoes': 6, 'small': 7}\n"
     ]
    }
   ],
   "source": [
    "sentence = 'My shoes are small, my feet are big.'\n",
    "\n",
    "dc = {s:i for i,s in enumerate(sorted(sentence.replace(',', '').split()))}\n",
    "print(dc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Assign index to each word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 6, 2, 7, 5, 4, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "sentence_int = torch.tensor([dc[s] for s in sentence.replace(',', '').split()])\n",
    "print(sentence_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, using the integer-vector representation of the input sentence, we can use an embedding layer to encode the inputs into a real-vector embedding. Here, we will use a 2-dimensional embedding such that each input word is represented by a 2-dimensional vector. Since the sentence consists of 8 words, this will result in a 8 X 2 dimensional embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3374, -0.1778],\n",
      "        [ 0.1794,  1.8951],\n",
      "        [ 0.3486,  0.6603],\n",
      "        [ 0.4954,  0.2692],\n",
      "        [ 0.6984, -1.4097],\n",
      "        [ 0.7671, -1.1925],\n",
      "        [ 0.3486,  0.6603],\n",
      "        [-0.2196, -0.3792]])\n",
      "torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123) #for reproducibility\n",
    "embed = torch.nn.Embedding(8, 2) #8 tokens, 2 dim vector\n",
    "embedded_sentence = embed(sentence_int).detach()\n",
    "\n",
    "print(embedded_sentence)\n",
    "print(embedded_sentence.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s discuss the widely utilized self-attention mechanism known as the scaled dot-product attention, which is integrated into the transformer architecture.\n",
    "\n",
    "Self-attention utilizes three weight matrices, referred to as $W_q$, $W_k$ and $W_v$ which are adjusted as model parameters during training. These matrices serve to project the inputs into query, key, and value components of the sequence, respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are computing the dot-product between the query and key vectors, these two vectors have to contain the same number of elements, However, the number of elements in the value vector $v^{(i)}$, which determines the size of the resulting context vector, is arbitrary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be extending the dimensions for query and keys to 3 and values to 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "print(embedded_sentence.shape) # [8, 2]\n",
    "d = embedded_sentence.shape[1] # [2]\n",
    "\n",
    "d_q, d_k, d_v = 3, 3, 4\n",
    "\n",
    "W_query = torch.nn.Parameter(torch.rand(d_q, d))  # Shape: [3, 2]\n",
    "W_key = torch.nn.Parameter(torch.rand(d_k, d))  # Shape: [3, 2]\n",
    "W_value = torch.nn.Parameter(torch.rand(d_v, d)) # Shape: [4, 2]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_sentence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.2961, 0.5166],\n",
      "        [0.2517, 0.6886],\n",
      "        [0.0740, 0.8665]], requires_grad=True)\n",
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "print(W_query)\n",
    "print(W_query.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.1366, 0.1025],\n",
      "        [0.1841, 0.7264],\n",
      "        [0.3153, 0.6871]], requires_grad=True)\n",
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "print(W_key)\n",
    "print(W_key.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.0756, 0.1966],\n",
      "        [0.3164, 0.4017],\n",
      "        [0.1186, 0.8274],\n",
      "        [0.3821, 0.6605]], requires_grad=True)\n",
      "torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "print(W_value)\n",
    "print(W_value.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Attention Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s suppose we are interested in computing the attention-vector for the second input element – the second input element acts as the query here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_2: tensor([0.1794, 1.8951]) \n",
      " x_2.shape: torch.Size([2])\n",
      "W_query: Parameter containing:\n",
      "tensor([[0.2961, 0.5166],\n",
      "        [0.2517, 0.6886],\n",
      "        [0.0740, 0.8665]], requires_grad=True) \n",
      " ... query_2: tensor([1.0321, 1.3501, 1.6555], grad_fn=<MvBackward0>) \n",
      "\n",
      "W_key: Parameter containing:\n",
      "tensor([[0.1366, 0.1025],\n",
      "        [0.1841, 0.7264],\n",
      "        [0.3153, 0.6871]], requires_grad=True) \n",
      " ... key_2: tensor([0.2187, 1.4097, 1.3587], grad_fn=<MvBackward0>) \n",
      "\n",
      "W_value: Parameter containing:\n",
      "tensor([[0.0756, 0.1966],\n",
      "        [0.3164, 0.4017],\n",
      "        [0.1186, 0.8274],\n",
      "        [0.3821, 0.6605]], requires_grad=True) \n",
      " ... value_2: tensor([0.3862, 0.8181, 1.5893, 1.3203], grad_fn=<MvBackward0>) \n",
      "\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "x_2 = embedded_sentence[1]\n",
    "query_2 = W_query.matmul(x_2)\n",
    "key_2 = W_key.matmul(x_2)\n",
    "value_2 = W_value.matmul(x_2)\n",
    "\n",
    "print(f\"x_2: {x_2} \\n x_2.shape: {x_2.shape}\")\n",
    "print(f\"W_query: {W_query} \\n ... query_2: {query_2} \\n\")\n",
    "print(f\"W_key: {W_key} \\n ... key_2: {key_2} \\n\")\n",
    "print(f\"W_value: {W_value} \\n ... value_2: {value_2} \\n\")\n",
    "print(query_2.shape)\n",
    "print(key_2.shape)\n",
    "print(value_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0321289999999999"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the matmul \n",
    "(0.1794*0.2961) + (1.8951*0.5166)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- These three matrices are used to project the embedded input tokens, $x^{(i)}$, into query, key, and value vectors via matrix multiplication:\n",
    "\n",
    "  - Query vector: $q^{(i)} = W_q \\,x^{(i)}$\n",
    "  - Key vector: $k^{(i)} = W_k \\,x^{(i)}$\n",
    "  - Value vector: $v^{(i)} = W_v \\,x^{(i)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queries.shape torch.Size([8, 3])\n",
      "keys.shape: torch.Size([8, 3])\n",
      "values.shape: torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "# Compute Query, Key, and Value vectors\n",
    "queries = W_query.matmul(embedded_sentence.T).T\n",
    "keys = W_key.matmul(embedded_sentence.T).T\n",
    "values = W_value.matmul(embedded_sentence.T).T\n",
    "\n",
    "print(\"queries.shape\", queries.shape) # [8, 3]\n",
    "print(\"keys.shape:\", keys.shape) # [8, 3]\n",
    "print(\"values.shape:\", values.shape) # [8, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0081, -0.0375, -0.1291],\n",
       "        [ 1.0321,  1.3501,  1.6555],\n",
       "        [ 0.4443,  0.5424,  0.5980],\n",
       "        [ 0.2858,  0.3100,  0.2699],\n",
       "        [-0.5214, -0.7949, -1.1699],\n",
       "        [-0.3889, -0.6280, -0.9766],\n",
       "        [ 0.4443,  0.5424,  0.5980],\n",
       "        [-0.2609, -0.3164, -0.3448]], grad_fn=<PermuteBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0279, -0.0671, -0.0158],\n",
       "        [ 0.2187,  1.4097,  1.3587],\n",
       "        [ 0.1153,  0.5439,  0.5636],\n",
       "        [ 0.0953,  0.2867,  0.3412],\n",
       "        [-0.0491, -0.8956, -0.7485],\n",
       "        [-0.0174, -0.7251, -0.5775],\n",
       "        [ 0.1153,  0.5439,  0.5636],\n",
       "        [-0.0689, -0.3159, -0.3298]], grad_fn=<PermuteBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then generalize this to compute th remaining key, and value elements for all inputs as well, since we will need them in the next step when we compute the unnormalized attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0094,  0.0353, -0.1071,  0.0115],\n",
       "        [ 0.3862,  0.8181,  1.5893,  1.3203],\n",
       "        [ 0.1562,  0.3756,  0.5877,  0.5693],\n",
       "        [ 0.0904,  0.2649,  0.2815,  0.3671],\n",
       "        [-0.2244, -0.3454, -1.0836, -0.6643],\n",
       "        [-0.1765, -0.2364, -0.8957, -0.4945],\n",
       "        [ 0.1562,  0.3756,  0.5877,  0.5693],\n",
       "        [-0.0912, -0.2218, -0.3398, -0.3344]], grad_fn=<PermuteBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3374, -0.1778],\n",
       "        [ 0.1794,  1.8951],\n",
       "        [ 0.3486,  0.6603],\n",
       "        [ 0.4954,  0.2692],\n",
       "        [ 0.6984, -1.4097],\n",
       "        [ 0.7671, -1.1925],\n",
       "        [ 0.3486,  0.6603],\n",
       "        [-0.2196, -0.3792]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3374,  0.1794,  0.3486,  0.4954,  0.6984,  0.7671,  0.3486, -0.2196],\n",
       "        [-0.1778,  1.8951,  0.6603,  0.2692, -1.4097, -1.1925,  0.6603, -0.3792]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_sentence.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the unnormalized attention weights  $\\omega$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we compute $\\omega_{ij}$ as the dot product between the query and key sequences,\n",
    "$\\omega_{ij}$ = q^{(i)}k^{(j)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we can compute the unnormalized attention weight for the query and 5th input element (corresponding to index position 4) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0321, 1.3501, 1.6555], grad_fn=<MvBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0279, -0.0671, -0.0158],\n",
       "        [ 0.2187,  1.4097,  1.3587],\n",
       "        [ 0.1153,  0.5439,  0.5636],\n",
       "        [ 0.0953,  0.2867,  0.3412],\n",
       "        [-0.0491, -0.8956, -0.7485],\n",
       "        [-0.0174, -0.7251, -0.5775],\n",
       "        [ 0.1153,  0.5439,  0.5636],\n",
       "        [-0.0689, -0.3159, -0.3298]], grad_fn=<PermuteBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.49896742\n"
     ]
    }
   ],
   "source": [
    "# query_2.dot(keys[4])\n",
    "test_omega_24 = -0.0491 * 1.0321 + -0.8956 * 1.3501 +  -0.7485 * 1.6555\n",
    "print(test_omega_24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.08795302000000002\n"
     ]
    }
   ],
   "source": [
    "#omaega_20 = [0.0279,-0.0671,-0.0158] * [1.0321, 1.3501, 1.6555]\n",
    "\n",
    "omega_20 = 0.0279 * 1.0321 + -0.0671 * 1.3501 + -0.0158 * 1.6555\n",
    "print(omega_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2.4988, grad_fn=<DotBackward0>)\n"
     ]
    }
   ],
   "source": [
    "omega_24 = query_2.dot(keys[4])\n",
    "print(omega_24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0879,  4.3783,  1.7863,  1.0502, -2.4988, -1.9530,  1.7863, -1.0434],\n",
      "       grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "omega_2 = query_2.matmul(keys.T)  # Matmul Query2[1, 3] *  Keys.T[3, 8] = [1, 8]\n",
    "print(omega_2) # Result matrix for query2, i.e. shoes  = [ 1 token, 8 dimensions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the Attention Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Attention Scores (Omega):\n",
      " tensor([[-0.0879,  4.3783,  1.7863,  1.0502, -2.4988, -1.9530,  1.7863, -1.0434]],\n",
      "       grad_fn=<MmBackward0>)\n",
      "Attention Weights (Softmax):\n",
      " tensor([[0.0432, 0.5687, 0.1273, 0.0832, 0.0107, 0.0147, 0.1273, 0.0249]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Compute attention scores for a specific token (e.g., \"shoes\")\n",
    "query_2 = queries[1].unsqueeze(0)  # Query for \"shoes\" (2nd token)\n",
    "omega_2 = query_2.matmul(keys.T)   # Dot product between query and keys\n",
    "print(\"Raw Attention Scores (Omega):\\n\", omega_2)\n",
    "\n",
    "# Scale and apply softmax\n",
    "attention_weights_2 = F.softmax(omega_2 / d_k**0.5, dim=1)\n",
    "print(\"Attention Weights (Softmax):\\n\", attention_weights_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0094,  0.0353, -0.1071,  0.0115],\n",
       "        [ 0.3862,  0.8181,  1.5893,  1.3203],\n",
       "        [ 0.1562,  0.3756,  0.5877,  0.5693],\n",
       "        [ 0.0904,  0.2649,  0.2815,  0.3671],\n",
       "        [-0.2244, -0.3454, -1.0836, -0.6643],\n",
       "        [-0.1765, -0.2364, -0.8957, -0.4945],\n",
       "        [ 0.1562,  0.3756,  0.5877,  0.5693],\n",
       "        [-0.0912, -0.2218, -0.3398, -0.3344]], grad_fn=<PermuteBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8])\n",
      "torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "print(attention_weights_2.shape)\n",
    "print(values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Vector Shape: torch.Size([1, 4])\n",
      "Context Vector:\n",
      " tensor([[0.2593, 0.5718, 1.0390, 0.9041]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Compute context vector for \"shoes\"\n",
    "context_vector_2 = attention_weights_2.matmul(values) # # matmul = attention_weights_2[1, 8] * values[8, 4]\n",
    "print(\"Context Vector Shape:\", context_vector_2.shape)  # [1, 4]\n",
    "print(\"Context Vector:\\n\", context_vector_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [0.0432, 0.5687, 0.1273, 0.0832, 0.0107, 0.0147, 0.1273, 0.0249] * tensor([[-0.0094,  0.0353, -0.1071,  0.0115],\n",
    "#                                                                            [ 0.3862,  0.8181,  1.5893,  1.3203],\n",
    "#                                                                            [ 0.1562,  0.3756,  0.5877,  0.5693],\n",
    "#                                                                            [ 0.0904,  0.2649,  0.2815,  0.3671],\n",
    "#                                                                            [-0.2244, -0.3454, -1.0836, -0.6643],\n",
    "#                                                                            [-0.1765, -0.2364, -0.8957, -0.4945],\n",
    "#                                                                            [ 0.1562,  0.3756,  0.5877,  0.5693],\n",
    "#                                                                            [-0.0912, -0.2218, -0.3398, -0.3344]]\n",
    "\n",
    "context_vector_query2_1 = (0.0432 * -0.0094) + (0.5687 *  0.3862) +  (0.1273 * 0.1562) + (0.0832 * 0.0904) + (0.0107 * -0.2244) + (0.0147 * -0.1765) + (0.1273 * 0.1562) + (0.0249 * -0.0912)\n",
    "context_vector_query2_2 = (0.0432 * 0.0353) + (0.5687 *  0.8181) +  (0.1273 * 0.3756) + (0.0832 * 0.2649) + (0.0107 * -0.3454) + (0.0147 * -0.2364) + (0.1273 * 0.3756) + (0.0249 * -0.2218)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25924915, 0.57175219\n"
     ]
    }
   ],
   "source": [
    "print(f\"{context_vector_query2_1}, {context_vector_query2_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model=2, num_heads=2):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        \n",
    "        # Linear layers for Q, K, V\n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Final output projection (after concatenating heads)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Tensor of shape (batch_size, seq_len, d_model)\n",
    "        Returns: Tensor of shape (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # 1) Compute Q, K, V => each shape: (batch_size, seq_len, d_model)\n",
    "        Q = self.W_Q(x)  # [1, 8, 2]\n",
    "        K = self.W_K(x)  # [1, 8, 2]\n",
    "        V = self.W_V(x)  # [1, 8, 2]\n",
    "        print(\"Before reshaping:\")\n",
    "        print(f\"Q: {Q} \\nK: {K} \\nV:{V}\")\n",
    "        \n",
    "        # 2) Reshape for multi-head attention:\n",
    "        #    (batch_size, seq_len, num_heads, head_dim) => then transpose\n",
    "        #    to (batch_size, num_heads, seq_len, head_dim)\n",
    "        Q = Q.reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        # Now Q, K, V = (1, 2, 8, 1)\n",
    "        print(\"After reshaping:\")\n",
    "        print(f\"Q: {Q} \\nK: {K} \\nV:{V}\")\n",
    "        \n",
    "        # 3) Compute attention scores: Q x K^T, scaled by sqrt(head_dim)\n",
    "        #    (batch_size, num_heads, seq_len, head_dim) x\n",
    "        #    (batch_size, num_heads, head_dim, seq_len) => (batch_size, num_heads, seq_len, seq_len)\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / (self.head_dim ** 0.5)  # => [1, 2, 8, 8]\n",
    "        print(\"scores:\", scores)\n",
    "        \n",
    "        # 4) Softmax along the \"seq_len\" of the keys\n",
    "        attn_weights = F.softmax(scores, dim=-1)  # => [1, 2, 8, 8]\n",
    "        print(\"attn_weights:\", attn_weights)\n",
    "        \n",
    "        # 5) Compute context: multiply attn_weights by V\n",
    "        context = torch.matmul(attn_weights, V)  # => [1, 2, 8, 1]\n",
    "        print(\"context (matmul by V):\", context)\n",
    "        \n",
    "        # 6) Transpose back + reshape to (batch_size, seq_len, d_model)\n",
    "        context = context.transpose(1, 2).reshape(batch_size, seq_len, d_model)  # => [1, 8, 2]\n",
    "        print(\"context (after reshape):\", context)\n",
    "        \n",
    "        # 7) Final projection to d_model\n",
    "        out = self.out_proj(context)  # => [1, 8, 2]\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3374, -0.1778],\n",
      "        [ 0.1794,  1.8951],\n",
      "        [ 0.3486,  0.6603],\n",
      "        [ 0.4954,  0.2692],\n",
      "        [ 0.6984, -1.4097],\n",
      "        [ 0.7671, -1.1925],\n",
      "        [ 0.3486,  0.6603],\n",
      "        [-0.2196, -0.3792]])\n",
      "tensor([[[ 0.3374, -0.1778],\n",
      "         [ 0.1794,  1.8951],\n",
      "         [ 0.3486,  0.6603],\n",
      "         [ 0.4954,  0.2692],\n",
      "         [ 0.6984, -1.4097],\n",
      "         [ 0.7671, -1.1925],\n",
      "         [ 0.3486,  0.6603],\n",
      "         [-0.2196, -0.3792]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 2])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(embedded_sentence)\n",
    "# Make it batch-size=1 => shape (1, 8, 2)\n",
    "x_in = embedded_sentence.unsqueeze(0)  # => (1,8,2)\n",
    "# adding another dimension\n",
    "print(x_in)\n",
    "x_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before reshaping:\n",
      "Q: tensor([[[-0.1736,  0.1679],\n",
      "         [ 0.0204,  1.5522],\n",
      "         [-0.0576,  0.7421],\n",
      "         [-0.0357,  0.5035],\n",
      "         [-0.1554, -0.6032],\n",
      "         [-0.0924, -0.4416],\n",
      "         [-0.0576,  0.7421],\n",
      "         [-0.4787, -0.0773]]], grad_fn=<ViewBackward0>) \n",
      "K: tensor([[[ 0.4076,  0.6441],\n",
      "         [ 1.5049, -0.7966],\n",
      "         [ 0.8276,  0.1040],\n",
      "         [ 0.5838,  0.4420],\n",
      "         [-0.3285,  1.6511],\n",
      "         [-0.2403,  1.5483],\n",
      "         [ 0.8276,  0.1040],\n",
      "         [ 0.4811,  0.4608]]], grad_fn=<ViewBackward0>) \n",
      "V:tensor([[[-0.5458,  0.0120],\n",
      "         [ 0.0089, -0.2744],\n",
      "         [-0.2984, -0.0596],\n",
      "         [-0.3670,  0.0632],\n",
      "         [-0.7932,  0.3392],\n",
      "         [-0.7088,  0.3594],\n",
      "         [-0.2984, -0.0596],\n",
      "         [-0.7761, -0.2966]]], grad_fn=<ViewBackward0>)\n",
      "After reshaping:\n",
      "Q: tensor([[[[-0.1736],\n",
      "          [ 0.0204],\n",
      "          [-0.0576],\n",
      "          [-0.0357],\n",
      "          [-0.1554],\n",
      "          [-0.0924],\n",
      "          [-0.0576],\n",
      "          [-0.4787]],\n",
      "\n",
      "         [[ 0.1679],\n",
      "          [ 1.5522],\n",
      "          [ 0.7421],\n",
      "          [ 0.5035],\n",
      "          [-0.6032],\n",
      "          [-0.4416],\n",
      "          [ 0.7421],\n",
      "          [-0.0773]]]], grad_fn=<TransposeBackward0>) \n",
      "K: tensor([[[[ 0.4076],\n",
      "          [ 1.5049],\n",
      "          [ 0.8276],\n",
      "          [ 0.5838],\n",
      "          [-0.3285],\n",
      "          [-0.2403],\n",
      "          [ 0.8276],\n",
      "          [ 0.4811]],\n",
      "\n",
      "         [[ 0.6441],\n",
      "          [-0.7966],\n",
      "          [ 0.1040],\n",
      "          [ 0.4420],\n",
      "          [ 1.6511],\n",
      "          [ 1.5483],\n",
      "          [ 0.1040],\n",
      "          [ 0.4608]]]], grad_fn=<TransposeBackward0>) \n",
      "V:tensor([[[[-0.5458],\n",
      "          [ 0.0089],\n",
      "          [-0.2984],\n",
      "          [-0.3670],\n",
      "          [-0.7932],\n",
      "          [-0.7088],\n",
      "          [-0.2984],\n",
      "          [-0.7761]],\n",
      "\n",
      "         [[ 0.0120],\n",
      "          [-0.2744],\n",
      "          [-0.0596],\n",
      "          [ 0.0632],\n",
      "          [ 0.3392],\n",
      "          [ 0.3594],\n",
      "          [-0.0596],\n",
      "          [-0.2966]]]], grad_fn=<TransposeBackward0>)\n",
      "scores: tensor([[[[-0.0708, -0.2613, -0.1437, -0.1014,  0.0570,  0.0417, -0.1437,\n",
      "           -0.0835],\n",
      "          [ 0.0083,  0.0308,  0.0169,  0.0119, -0.0067, -0.0049,  0.0169,\n",
      "            0.0098],\n",
      "          [-0.0235, -0.0867, -0.0477, -0.0336,  0.0189,  0.0138, -0.0477,\n",
      "           -0.0277],\n",
      "          [-0.0146, -0.0537, -0.0296, -0.0208,  0.0117,  0.0086, -0.0296,\n",
      "           -0.0172],\n",
      "          [-0.0634, -0.2339, -0.1286, -0.0907,  0.0511,  0.0374, -0.1286,\n",
      "           -0.0748],\n",
      "          [-0.0377, -0.1391, -0.0765, -0.0540,  0.0304,  0.0222, -0.0765,\n",
      "           -0.0445],\n",
      "          [-0.0235, -0.0867, -0.0477, -0.0336,  0.0189,  0.0138, -0.0477,\n",
      "           -0.0277],\n",
      "          [-0.1951, -0.7204, -0.3962, -0.2795,  0.1572,  0.1150, -0.3962,\n",
      "           -0.2303]],\n",
      "\n",
      "         [[ 0.1081, -0.1337,  0.0175,  0.0742,  0.2772,  0.2599,  0.0175,\n",
      "            0.0774],\n",
      "          [ 0.9997, -1.2364,  0.1615,  0.6860,  2.5628,  2.4032,  0.1615,\n",
      "            0.7153],\n",
      "          [ 0.4780, -0.5911,  0.0772,  0.3280,  1.2253,  1.1490,  0.0772,\n",
      "            0.3420],\n",
      "          [ 0.3243, -0.4011,  0.0524,  0.2225,  0.8313,  0.7796,  0.0524,\n",
      "            0.2320],\n",
      "          [-0.3885,  0.4805, -0.0627, -0.2666, -0.9959, -0.9339, -0.0627,\n",
      "           -0.2780],\n",
      "          [-0.2844,  0.3518, -0.0459, -0.1952, -0.7292, -0.6838, -0.0459,\n",
      "           -0.2035],\n",
      "          [ 0.4780, -0.5911,  0.0772,  0.3280,  1.2253,  1.1490,  0.0772,\n",
      "            0.3420],\n",
      "          [-0.0498,  0.0616, -0.0080, -0.0342, -0.1276, -0.1197, -0.0080,\n",
      "           -0.0356]]]], grad_fn=<DivBackward0>)\n",
      "attn_weights: tensor([[[[0.1266, 0.1046, 0.1177, 0.1228, 0.1439, 0.1417, 0.1177, 0.1250],\n",
      "          [0.1247, 0.1276, 0.1258, 0.1252, 0.1229, 0.1231, 0.1258, 0.1249],\n",
      "          [0.1257, 0.1180, 0.1227, 0.1244, 0.1311, 0.1304, 0.1227, 0.1251],\n",
      "          [0.1254, 0.1206, 0.1236, 0.1246, 0.1288, 0.1284, 0.1236, 0.1251],\n",
      "          [0.1265, 0.1067, 0.1185, 0.1231, 0.1418, 0.1399, 0.1185, 0.1251],\n",
      "          [0.1260, 0.1138, 0.1212, 0.1240, 0.1349, 0.1338, 0.1212, 0.1251],\n",
      "          [0.1257, 0.1180, 0.1227, 0.1244, 0.1311, 0.1304, 0.1227, 0.1251],\n",
      "          [0.1266, 0.0749, 0.1036, 0.1164, 0.1801, 0.1727, 0.1036, 0.1222]],\n",
      "\n",
      "         [[0.1266, 0.0994, 0.1157, 0.1224, 0.1500, 0.1474, 0.1157, 0.1228],\n",
      "          [0.0813, 0.0087, 0.0352, 0.0594, 0.3881, 0.3309, 0.0352, 0.0612],\n",
      "          [0.1179, 0.0405, 0.0789, 0.1015, 0.2489, 0.2306, 0.0789, 0.1029],\n",
      "          [0.1241, 0.0601, 0.0945, 0.1121, 0.2060, 0.1956, 0.0945, 0.1131],\n",
      "          [0.1050, 0.2503, 0.1454, 0.1186, 0.0572, 0.0608, 0.1454, 0.1173],\n",
      "          [0.1121, 0.2119, 0.1423, 0.1226, 0.0719, 0.0752, 0.1423, 0.1216],\n",
      "          [0.1179, 0.0405, 0.0789, 0.1015, 0.2489, 0.2306, 0.0789, 0.1029],\n",
      "          [0.1236, 0.1382, 0.1289, 0.1255, 0.1143, 0.1153, 0.1289, 0.1254]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "context (matmul by V): tensor([[[[-0.4950],\n",
      "          [-0.4696],\n",
      "          [-0.4799],\n",
      "          [-0.4770],\n",
      "          [-0.4927],\n",
      "          [-0.4845],\n",
      "          [-0.4799],\n",
      "          [-0.5330]],\n",
      "\n",
      "         [[ 0.0356],\n",
      "          [ 0.2306],\n",
      "          [ 0.1241],\n",
      "          [ 0.0874],\n",
      "          [-0.0708],\n",
      "          [-0.0507],\n",
      "          [ 0.1241],\n",
      "          [-0.0008]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "context (after reshape): tensor([[[-0.4950,  0.0356],\n",
      "         [-0.4696,  0.2306],\n",
      "         [-0.4799,  0.1241],\n",
      "         [-0.4770,  0.0874],\n",
      "         [-0.4927, -0.0708],\n",
      "         [-0.4845, -0.0507],\n",
      "         [-0.4799,  0.1241],\n",
      "         [-0.5330, -0.0008]]], grad_fn=<ReshapeAliasBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Make it batch-size=1 => shape (1, 8, 2)\n",
    "x_in = embedded_sentence.unsqueeze(0)  # => (1,8,2)\n",
    "\n",
    "mha = MultiHeadSelfAttention(d_model=2, num_heads=2)\n",
    "mha_out = mha(x_in)  # => (1,8,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- After Multi-Head Self-Attention --\n",
      "MHA output shape: torch.Size([1, 8, 2])\n",
      "MHA output: tensor([[[0.1839, 0.7218],\n",
      "         [0.1803, 0.6445],\n",
      "         [0.1805, 0.6862],\n",
      "         [0.1774, 0.6997],\n",
      "         [0.1777, 0.7618],\n",
      "         [0.1746, 0.7531],\n",
      "         [0.1805, 0.6862],\n",
      "         [0.2011, 0.7406]]], grad_fn=<AddBackward0>)\n",
      "Add & Norm shape: torch.Size([1, 8, 2])\n",
      "Add & Norm: tensor([[[-0.9634,  0.9634],\n",
      "         [-1.0000,  1.0000],\n",
      "         [-1.0000,  1.0000],\n",
      "         [-0.9998,  0.9998],\n",
      "         [ 1.0000, -1.0000],\n",
      "         [ 1.0000, -1.0000],\n",
      "         [-1.0000,  1.0000],\n",
      "         [-0.9999,  0.9999]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Residual + Norm\n",
    "residual_1 = x_in + mha_out\n",
    "layer_norm_1 = nn.LayerNorm(normalized_shape=2)\n",
    "normed_1 = layer_norm_1(residual_1)\n",
    "\n",
    "print(\"\\n-- After Multi-Head Self-Attention --\")\n",
    "print(\"MHA output shape:\", mha_out.shape)\n",
    "print(\"MHA output:\", mha_out)\n",
    "print(\"Add & Norm shape:\", normed_1.shape)\n",
    "print(\"Add & Norm:\", normed_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ffn FeedForward(\n",
      "  (linear1): Linear(in_features=2, out_features=4, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (linear2): Linear(in_features=4, out_features=2, bias=True)\n",
      ")\n",
      "ffn_out tensor([[[-0.0663,  0.2402],\n",
      "         [-0.0638,  0.2437],\n",
      "         [-0.0638,  0.2437],\n",
      "         [-0.0638,  0.2437],\n",
      "         [-0.2012, -0.0018],\n",
      "         [-0.2012, -0.0018],\n",
      "         [-0.0638,  0.2437],\n",
      "         [-0.0638,  0.2437]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model=2, hidden_dim=4):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_dim, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "ffn = FeedForward(d_model=2, hidden_dim=4)\n",
    "ffn_out = ffn(normed_1)  # => (1,8,2)\n",
    "print(\"ffn\", ffn)\n",
    "print(\"ffn_out\", ffn_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- After Feed-Forward --\n",
      "FFN output shape: torch.Size([1, 8, 2])\n",
      "Final encoder output shape: torch.Size([1, 8, 2])\n",
      "Final encoder output: tensor([[[-1.0000,  1.0000],\n",
      "         [-1.0000,  1.0000],\n",
      "         [-1.0000,  1.0000],\n",
      "         [-1.0000,  1.0000],\n",
      "         [ 1.0000, -1.0000],\n",
      "         [ 1.0000, -1.0000],\n",
      "         [-1.0000,  1.0000],\n",
      "         [-1.0000,  1.0000]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "residual_2 = normed_1 + ffn_out\n",
    "layer_norm_2 = nn.LayerNorm(normalized_shape=2)\n",
    "encoder_out = layer_norm_2(residual_2)\n",
    "\n",
    "print(\"\\n-- After Feed-Forward --\")\n",
    "print(\"FFN output shape:\", ffn_out.shape)\n",
    "print(\"Final encoder output shape:\", encoder_out.shape)\n",
    "print(\"Final encoder output:\", encoder_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go with 3 heads now. \n",
    "- d_q, d_k, d_v = 3, 3, 4\n",
    "- d = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vectors:  [tensor([[0.2593]], grad_fn=<MmBackward0>), tensor([[0.5718]], grad_fn=<MmBackward0>), tensor([[1.0390]], grad_fn=<MmBackward0>)]\n",
      "Combined Context Vector Shape: torch.Size([1, 3])\n",
      "Combined Context Vector:\n",
      " tensor([[0.2593, 0.5718, 1.0390]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Number of heads\n",
    "num_heads = 3\n",
    "\n",
    "# Split embeddings into multiple heads\n",
    "head_dim = d_v // num_heads  # Dimension per head\n",
    "heads = [values[:, i*head_dim:(i+1)*head_dim] for i in range(num_heads)]\n",
    "\n",
    "# Compute context vectors for each head\n",
    "context_vectors = []\n",
    "for head in heads:\n",
    "    # Compute attention scores (simplified for illustration)\n",
    "    attention_scores = F.softmax(query_2.matmul(keys.T) / d_k**0.5, dim=1)\n",
    "    context_vector = attention_scores.matmul(head)\n",
    "    context_vectors.append(context_vector)\n",
    "\n",
    "print(\"context_vectors: \", context_vectors)\n",
    "# Concatenate context vectors from all heads\n",
    "combined_context = torch.cat(context_vectors, dim=1)\n",
    "print(\"Combined Context Vector Shape:\", combined_context.shape)  # [1, 4]\n",
    "print(\"Combined Context Vector:\\n\", combined_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.0094],\n",
       "         [ 0.3862],\n",
       "         [ 0.1562],\n",
       "         [ 0.0904],\n",
       "         [-0.2244],\n",
       "         [-0.1765],\n",
       "         [ 0.1562],\n",
       "         [-0.0912]], grad_fn=<SliceBackward0>),\n",
       " tensor([[ 0.0353],\n",
       "         [ 0.8181],\n",
       "         [ 0.3756],\n",
       "         [ 0.2649],\n",
       "         [-0.3454],\n",
       "         [-0.2364],\n",
       "         [ 0.3756],\n",
       "         [-0.2218]], grad_fn=<SliceBackward0>),\n",
       " tensor([[-0.1071],\n",
       "         [ 1.5893],\n",
       "         [ 0.5877],\n",
       "         [ 0.2815],\n",
       "         [-1.0836],\n",
       "         [-0.8957],\n",
       "         [ 0.5877],\n",
       "         [-0.3398]], grad_fn=<SliceBackward0>)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 3\n",
    "multihead_W_query = torch.nn.Parameter(torch.rand(h, d_q, d))\n",
    "multihead_W_key = torch.nn.Parameter(torch.rand(h, d_k, d))\n",
    "multihead_W_value = torch.nn.Parameter(torch.rand(h, d_v, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[0.8536, 0.5932],\n",
      "         [0.6367, 0.9826],\n",
      "         [0.2745, 0.6584]],\n",
      "\n",
      "        [[0.2775, 0.8573],\n",
      "         [0.8993, 0.0390],\n",
      "         [0.9268, 0.7388]],\n",
      "\n",
      "        [[0.7179, 0.7058],\n",
      "         [0.9156, 0.4340],\n",
      "         [0.0772, 0.3565]]], requires_grad=True)\n",
      "torch.Size([3, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "print(multihead_W_query)\n",
    "print(multihead_W_query.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(here, let’s keep the focus on the 3rd element corresponding to index position 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1794, 1.8951])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.2772, 1.9764, 1.2970],\n",
      "        [1.6745, 0.2353, 1.5663],\n",
      "        [1.4664, 0.9867, 0.6895]], grad_fn=<UnsafeViewBackward0>)\n",
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "multihead_query_2 = multihead_W_query.matmul(x_2)\n",
    "print(multihead_query_2)\n",
    "print(multihead_query_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys:\n",
      "tensor([[1.0367, 0.5123, 1.9268],\n",
      "        [1.0603, 1.1722, 1.6966],\n",
      "        [1.2750, 1.4245, 0.1450]], grad_fn=<UnsafeViewBackward0>)\n",
      "torch.Size([3, 3])\n",
      "Values:\n",
      "tensor([[1.7906, 0.2750, 1.8344, 0.3146],\n",
      "        [1.0396, 0.9122, 1.3936, 1.6952],\n",
      "        [0.2099, 1.0958, 0.1481, 1.7002]], grad_fn=<UnsafeViewBackward0>)\n",
      "torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "multihead_key_2 = multihead_W_key.matmul(x_2)\n",
    "multihead_value_2 = multihead_W_value.matmul(x_2)\n",
    "print(\"Keys:\")\n",
    "print(multihead_key_2)\n",
    "print(multihead_key_2.shape)\n",
    "print(\"Values:\")\n",
    "print(multihead_value_2)\n",
    "print(multihead_value_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, these key and value elements are specific to the query element. But, similar to earlier, we will also need the value and keys for the other sequence elements in order to compute the attention scores for the query. We can do this is by expanding the input sequence embeddings to size 3, i.e., the number of attention heads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3374, -0.1778],\n",
      "        [ 0.1794,  1.8951],\n",
      "        [ 0.3486,  0.6603],\n",
      "        [ 0.4954,  0.2692],\n",
      "        [ 0.6984, -1.4097],\n",
      "        [ 0.7671, -1.1925],\n",
      "        [ 0.3486,  0.6603],\n",
      "        [-0.2196, -0.3792]])\n",
      "torch.Size([8, 2])\n",
      "\n",
      "Transpose...\n",
      "tensor([[ 0.3374,  0.1794,  0.3486,  0.4954,  0.6984,  0.7671,  0.3486, -0.2196],\n",
      "        [-0.1778,  1.8951,  0.6603,  0.2692, -1.4097, -1.1925,  0.6603, -0.3792]])\n",
      "torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "print(embedded_sentence)\n",
    "print(embedded_sentence.shape)\n",
    "print(\"\\nTranspose...\")\n",
    "print(embedded_sentence.T)\n",
    "print(embedded_sentence.T.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have 3 attention heads, we will duplicate the input embeddings to size 3.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.3374,  0.1794,  0.3486,  0.4954,  0.6984,  0.7671,  0.3486,\n",
      "          -0.2196],\n",
      "         [-0.1778,  1.8951,  0.6603,  0.2692, -1.4097, -1.1925,  0.6603,\n",
      "          -0.3792]],\n",
      "\n",
      "        [[ 0.3374,  0.1794,  0.3486,  0.4954,  0.6984,  0.7671,  0.3486,\n",
      "          -0.2196],\n",
      "         [-0.1778,  1.8951,  0.6603,  0.2692, -1.4097, -1.1925,  0.6603,\n",
      "          -0.3792]],\n",
      "\n",
      "        [[ 0.3374,  0.1794,  0.3486,  0.4954,  0.6984,  0.7671,  0.3486,\n",
      "          -0.2196],\n",
      "         [-0.1778,  1.8951,  0.6603,  0.2692, -1.4097, -1.1925,  0.6603,\n",
      "          -0.3792]]])\n",
      "torch.Size([3, 2, 8])\n"
     ]
    }
   ],
   "source": [
    "stacked_inputs = embedded_sentence.T.repeat(3, 1, 1)\n",
    "print(stacked_inputs)\n",
    "print(stacked_inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can compute all the keys and values using via `torch.bmm()` (batch matrix multiplication):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multihead_keys.shape: torch.Size([3, 3, 8])\n",
      "multihead_values.shape: torch.Size([3, 4, 8])\n"
     ]
    }
   ],
   "source": [
    "multihead_keys = torch.bmm(multihead_W_key, stacked_inputs)\n",
    "multihead_values = torch.bmm(multihead_W_value, stacked_inputs)\n",
    "print(\"multihead_keys.shape:\", multihead_keys.shape)\n",
    "print(\"multihead_values.shape:\", multihead_values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have tensors that represent the three attention heads in their first dimension. \n",
    "- The third dimension refer to the number of words, and \n",
    "- The second dimension refers to the embedding size, respectively. \n",
    "\n",
    "To make the values and keys more intuitive to interpret, we will swap the second and third dimensions, resulting in tensors with the same dimensional structure as the original input sequence, `embedded_sentence`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multihead_keys.shape: torch.Size([3, 8, 3])\n",
      "multihead_values.shape: torch.Size([3, 8, 4])\n"
     ]
    }
   ],
   "source": [
    "multihead_keys = multihead_keys.permute(0, 2, 1)\n",
    "multihead_values = multihead_values.permute(0, 2, 1)\n",
    "print(\"multihead_keys.shape:\", multihead_keys.shape)\n",
    "print(\"multihead_values.shape:\", multihead_values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, \n",
    "- the first dimension represents the number of heads\n",
    "- the second dimension represents the numebr of tokens\n",
    "- the thrid dimension represents the number of embedding size for keys and values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multihead_keys:\n",
      "tensor([[[-0.0449,  0.0960, -0.0198],\n",
      "         [ 1.0367,  0.5123,  1.9268],\n",
      "         [ 0.4035,  0.2948,  0.8014],\n",
      "         [ 0.2168,  0.2639,  0.4873],\n",
      "         [-0.6482, -0.0428, -1.0552],\n",
      "         [-0.5222,  0.0355, -0.8125],\n",
      "         [ 0.4035,  0.2948,  0.8014],\n",
      "         [-0.2346, -0.1772, -0.4690]],\n",
      "\n",
      "        [[ 0.0637,  0.0395,  0.1757],\n",
      "         [ 1.0603,  1.1722,  1.6966],\n",
      "         [ 0.5012,  0.5292,  0.8617],\n",
      "         [ 0.3671,  0.3648,  0.6854],\n",
      "         [-0.4056, -0.5210, -0.4756],\n",
      "         [-0.2619, -0.3663, -0.2355],\n",
      "         [ 0.5012,  0.5292,  0.8617],\n",
      "         [-0.2968, -0.3121, -0.5132]],\n",
      "\n",
      "        [[ 0.1204,  0.0852,  0.1406],\n",
      "         [ 1.2750,  1.4245,  0.1450],\n",
      "         [ 0.6381,  0.6731,  0.1751],\n",
      "         [ 0.4995,  0.4927,  0.2252],\n",
      "         [-0.3849, -0.5457,  0.2543],\n",
      "         [-0.2061, -0.3526,  0.2919],\n",
      "         [ 0.6381,  0.6731,  0.1751],\n",
      "         [-0.3796, -0.3985, -0.1090]]], grad_fn=<PermuteBackward0>)\n",
      "multihead_values:\n",
      "tensor([[[-0.1004,  0.1619,  0.0330, -0.0198],\n",
      "         [ 1.7906,  0.2750,  1.8344,  0.3146],\n",
      "         [ 0.6785,  0.2474,  0.8048,  0.1175],\n",
      "         [ 0.3440,  0.2881,  0.5326,  0.0576],\n",
      "         [-1.1733,  0.2362, -0.8830, -0.2111],\n",
      "         [-0.9588,  0.2932, -0.6449, -0.1737],\n",
      "         [ 0.6785,  0.2474,  0.8048,  0.1175],\n",
      "         [-0.3933, -0.1524, -0.4734, -0.0680]],\n",
      "\n",
      "        [[ 0.0090,  0.0502, -0.1263,  0.1735],\n",
      "         [ 1.0396,  0.9122,  1.3936,  1.6952],\n",
      "         [ 0.4483,  0.4275,  0.4892,  0.8593],\n",
      "         [ 0.2891,  0.3098,  0.2038,  0.6820],\n",
      "         [-0.5230, -0.3597, -1.0262, -0.4800],\n",
      "         [-0.3894, -0.2366, -0.8658, -0.2405],\n",
      "         [ 0.4483,  0.4275,  0.4892,  0.8593],\n",
      "         [-0.2633, -0.2530, -0.2811, -0.5117]],\n",
      "\n",
      "        [[ 0.0320,  0.1478,  0.2358,  0.0071],\n",
      "         [ 0.2099,  1.0958,  0.1481,  1.7002],\n",
      "         [ 0.1149,  0.5843,  0.2533,  0.7269],\n",
      "         [ 0.0984,  0.4882,  0.3523,  0.4625],\n",
      "         [-0.0348, -0.2265,  0.4762, -0.8736],\n",
      "         [-0.0037, -0.0668,  0.5272, -0.6561],\n",
      "         [ 0.1149,  0.5843,  0.2533,  0.7269],\n",
      "         [-0.0688, -0.3493, -0.1592, -0.4266]]], grad_fn=<PermuteBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"multihead_keys:\")\n",
    "print(multihead_keys)\n",
    "\n",
    "print(\"multihead_values:\")\n",
    "print(multihead_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
