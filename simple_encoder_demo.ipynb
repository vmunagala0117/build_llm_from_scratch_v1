{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.5.1\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['my', 'shoes', 'are', 'small', 'my', 'feet', 'are', 'big.']\n",
      "Vocab: {'my': 0, 'shoes': 1, 'are': 2, 'small': 3, 'feet': 4, 'big.': 5}\n",
      "Token IDs (no batch): tensor([0, 1, 2, 3, 0, 4, 2, 5])\n",
      "Shape: torch.Size([8])\n",
      "\n",
      "Token IDs with batch dimension: tensor([[0, 1, 2, 3, 0, 4, 2, 5]])\n",
      "Shape: torch.Size([1, 8])\n",
      "\n",
      "Embedded Tokens with batch size = 1:\n",
      "tensor([[[ 0.3374, -0.1778, -0.3035, -0.5880],\n",
      "         [ 0.3486,  0.6603, -0.2196, -0.3792],\n",
      "         [-0.1606, -0.4015,  0.6957, -1.8061],\n",
      "         [ 1.8960, -0.1750,  1.3689, -1.6033],\n",
      "         [ 0.3374, -0.1778, -0.3035, -0.5880],\n",
      "         [-0.7849, -1.4096, -0.4076,  0.7953],\n",
      "         [-0.1606, -0.4015,  0.6957, -1.8061],\n",
      "         [ 0.9985,  0.2212,  1.8319, -0.3378]]], grad_fn=<EmbeddingBackward0>)\n",
      "Shape: torch.Size([1, 8, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(123) #for reproducibility\n",
    "\n",
    "# 1) Your naive tokenization\n",
    "sentence = \"my shoes are small, my feet are big.\"\n",
    "tokens = sentence.replace(\",\", \"\").split()\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# 2) Build vocab\n",
    "vocab = {}\n",
    "for tok in tokens:\n",
    "    if tok not in vocab:\n",
    "        vocab[tok] = len(vocab)\n",
    "print(\"Vocab:\", vocab)\n",
    "\n",
    "# 3) Convert tokens to IDs\n",
    "token_ids = [vocab[tok] for tok in tokens]\n",
    "token_ids_tensor = torch.tensor(token_ids)\n",
    "\n",
    "print(\"Token IDs (no batch):\", token_ids_tensor)\n",
    "print(\"Shape:\", token_ids_tensor.shape)  # [8]\n",
    "\n",
    "# 4) Add a batch dimension\n",
    "#    Now token_ids_tensor is [1, 8] instead of [8]\n",
    "token_ids_tensor = token_ids_tensor.unsqueeze(0)\n",
    "print(\"\\nToken IDs with batch dimension:\", token_ids_tensor)\n",
    "print(\"Shape:\", token_ids_tensor.shape)  # [1, 8]\n",
    "\n",
    "# 5) Create an embedding layer\n",
    "embedding_dim = 4\n",
    "embed = nn.Embedding(num_embeddings=len(vocab), embedding_dim=embedding_dim)\n",
    "\n",
    "# 6) Apply the embedding\n",
    "embedded_tokens = embed(token_ids_tensor)\n",
    "print(\"\\nEmbedded Tokens with batch size = 1:\")\n",
    "print(embedded_tokens)\n",
    "print(\"Shape:\", embedded_tokens.shape)  # [1, 8, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LearnedPositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple learned positional embedding for a maximum sequence length of `max_len`.\n",
    "    Each position i gets an embedding of size `d_model`.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(123) #for reproducibility\n",
    "        # Create an embedding layer for positions\n",
    "        self.pos_embedding = nn.Embedding(num_embeddings=max_len, embedding_dim=d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: A tensor of shape (batch_size, seq_len, d_model)\n",
    "           containing token embeddings.\n",
    "\n",
    "        Returns:\n",
    "        A tensor of shape (batch_size, seq_len, d_model),\n",
    "        where each token embedding is augmented with its\n",
    "        learned positional vector.\n",
    "        \"\"\"\n",
    "        # Get the sequence length from the input\n",
    "        seq_len = x.size(1)\n",
    "\n",
    "        # Create a range of positions: 0..seq_len-1\n",
    "        # shape: (1, seq_len)\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0)\n",
    "\n",
    "        # Retrieve the positional embeddings for these positions\n",
    "        # shape: (1, seq_len, d_model)\n",
    "        pos_emb = self.pos_embedding(positions)\n",
    "\n",
    "        # Add them to the original token embeddings\n",
    "        # Broadcasting over batch dimension\n",
    "        x = x + pos_emb\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 8, 4])\n",
      "Output shape: torch.Size([1, 8, 4])\n",
      "Positional Embeddings added to the input embedding: tensor([[[ 0.6747, -0.3556, -0.6071, -1.1760],\n",
      "         [ 0.6972,  1.3207, -0.4393, -0.7583],\n",
      "         [ 0.6065, -1.5940,  1.3941, -3.2158],\n",
      "         [ 2.0753,  1.7201,  1.8644, -1.3341],\n",
      "         [ 0.2603, -1.1982, -0.4725,  0.3298],\n",
      "         [ 0.7961, -0.1085,  0.8677,  0.5944],\n",
      "         [ 0.8019, -0.1523,  0.2112, -3.8990],\n",
      "         [ 0.1786, -0.1998,  0.8699,  0.9446]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "seq_len = len(tokens)\n",
    "d_model = embedding_dim\n",
    "max_len = 10\n",
    "\n",
    "# Initialize learned positional encoding\n",
    "learned_pe = LearnedPositionalEncoding(d_model=embedding_dim, max_len=max_len)\n",
    "\n",
    "# Forward pass\n",
    "x_with_pe = learned_pe(embedded_tokens)\n",
    "\n",
    "print(\"Input shape:\", embedded_tokens.shape)    # (1, 8, 4)\n",
    "print(\"Output shape:\", x_with_pe.shape)         # (1, 8, 4)\n",
    "print(\"Positional Embeddings added to the input embedding:\", x_with_pe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point:\n",
    "\n",
    "- Batch size = 1\n",
    "- Sequence length = 8 (tokens)\n",
    "- Embedding dim = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Self-Attention Head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Q/K/V Projections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s discuss the widely utilized self-attention mechanism known as the scaled dot-product attention, which is integrated into the transformer architecture.\n",
    "\n",
    "Self-attention utilizes three weight matrices, referred to as $W_q$, $W_k$ and $W_v$ which are adjusted as model parameters during training. These matrices serve to project the inputs into query, key, and value components of the sequence, respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are computing the dot-product between the query and key vectors, these two vectors have to contain the same number of elements, However, the number of elements in the value vector $v^{(i)}$, which determines the size of the resulting context vector, is arbitrary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll create small linear layers to transform from the embedding dimension (4) into Q, K, V. For simplicity, let’s make Q, K, and V all dimension = 4 as well (typical practice might use a smaller dimension if we wanted multi-head, but we’ll keep it direct here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "d_model = 4\n",
    "embedded_tokens = x_with_pe\n",
    "\n",
    "W_Q = nn.Linear(d_model, d_model, bias=False)\n",
    "W_K = nn.Linear(d_model, d_model, bias=False)\n",
    "W_V = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "# Transform to Q, K, V\n",
    "Q = W_Q(embedded_tokens)  # shape: [1, 8, 4]\n",
    "K = W_K(embedded_tokens)  # shape: [1, 8, 4]\n",
    "V = W_V(embedded_tokens)  # shape: [1, 8, 4]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_Q Linear(in_features=4, out_features=4, bias=False)\n",
      "Q tensor([[[-0.2145,  0.2703, -0.4016,  0.0485],\n",
      "         [-0.1542,  0.6481,  0.0180, -0.5414],\n",
      "         [-1.1026, -0.0709, -1.4118,  0.2862],\n",
      "         [-1.1092, -0.4009, -0.8602, -1.6137],\n",
      "         [ 0.1066, -0.5095, -0.2046,  0.3074],\n",
      "         [-0.2675, -0.9306, -0.3252, -0.5226],\n",
      "         [-0.9536,  1.0757, -1.0564,  0.0503],\n",
      "         [-0.0776, -0.8410, -0.0856, -0.2677]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"W_Q\", W_Q)\n",
    "print(\"Q\", Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Self-Attention for a Single Token: \"shoes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s isolate the second token (index 1) for “shoes.” Note that the entire Q/K/V are needed to compute attention across all tokens, but we’ll highlight the single query for “shoes”:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- These three matrices are used to project the embedded input tokens, $x^{(i)}$, into query, key, and value vectors via matrix multiplication:\n",
    "\n",
    "  - Query vector: $q^{(i)} = W_q \\,x^{(i)}$\n",
    "  - Key vector: $k^{(i)} = W_k \\,x^{(i)}$\n",
    "  - Value vector: $v^{(i)} = W_v \\,x^{(i)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shoes_query tensor([[-0.1542,  0.6481,  0.0180, -0.5414]], grad_fn=<UnsqueezeBackward0>)\n",
      "\n",
      "Attention Scores (shoes): tensor([[-0.0820,  0.1623, -0.5437,  0.0661, -0.0908,  0.0062, -0.3285,  0.0021]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Attention Weights (shoes): tensor([[0.1247, 0.1592, 0.0786, 0.1446, 0.1236, 0.1362, 0.0975, 0.1356]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# \"shoes\" is token index 1\n",
    "shoes_query = Q[0, 1, :]  # shape: [4], we drop the batch dimension index 0\n",
    "shoes_query = shoes_query.unsqueeze(0)  # shape: [1, 4] -> (1, d_model)\n",
    "print(\"shoes_query\", shoes_query)\n",
    "\n",
    "# Keys: shape [1, 8, 4] -> we drop batch dimension => shape: [8, 4]\n",
    "all_keys = K[0]  # shape: [8, 4]\n",
    "all_values = V[0]  # shape: [8, 4]\n",
    "\n",
    "# 1) Dot product between query and all keys => shape: [1, 4] x [8, 4]^T = [1, 8]\n",
    "attn_scores = shoes_query.matmul(all_keys.transpose(0,1))  # => shape: [1, 8]\n",
    "\n",
    "# 2) Scale by sqrt(d_model)\n",
    "attn_scores = attn_scores / (d_model**0.5)\n",
    "\n",
    "# 3) Softmax\n",
    "attn_weights = F.softmax(attn_scores, dim=-1)  # shape: [1, 8]\n",
    "\n",
    "print(\"\\nAttention Scores (shoes):\", attn_scores)\n",
    "print(\"Attention Weights (shoes):\", attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, attn_weights is a distribution over the 8 tokens, telling us how strongly “shoes” attends to each token (including itself)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Vector for “shoes”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Context vector for 'shoes': tensor([[ 0.4238, -0.5244, -0.5151, -0.1296]], grad_fn=<MmBackward0>)\n",
      "Shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "# shape: [1, 8] x [8, 4] = [1, 4]\n",
    "shoes_context = attn_weights.matmul(all_values)\n",
    "print(\"\\nContext vector for 'shoes':\", shoes_context)\n",
    "print(\"Shape:\", shoes_context.shape)  # [1, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Interpretation:</b>\n",
    "\n",
    "- `shoes_context` is now the 4-dimensional “refined” embedding that incorporates relationships to all other tokens.\n",
    "- If `attn_weights` is high for certain tokens (e.g., “small”), that token’s Value vector influences the final context more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `attn_scores` will likely be some random distribution because W_Q, W_K, W_V are uninitialized.\n",
    "- After softmax, `attn_weights` might highlight certain tokens more (purely by random chance in this untrained scenario).\n",
    "- The `shoes_context` is a 4D vector combining Value vectors from all tokens in proportion to these attention weights.\n",
    "- In a trained setting, you’d see “shoes” paying higher attention to tokens like “small,” or maybe repeating “my,” reflecting learned semantics. But this snippet demonstrates exactly how to:\n",
    "\n",
    "1. Get a single query for “shoes.”\n",
    "2. Dot it with all keys.\n",
    "3. Softmax → attention weights.\n",
    "4. Weighted sum of Values → context vector for “shoes.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a toy multi-head attention example in PyTorch, building on the previous single-head demo. We’ll assume:\n",
    "\n",
    "- 𝑑_model=4\n",
    "- num_heads=2\n",
    "- head_dim= 𝑑_model/num_heads = 2\n",
    "\n",
    "\n",
    "We’ll define a mini MultiHeadSelfAttention class that:\n",
    "\n",
    "- Splits 𝑄,𝐾,𝑉 into heads,\n",
    "- Performs scaled dot-product attention per head,\n",
    "- Concatenates the results,\n",
    "- Applies a final output projection.\n",
    "\n",
    "Then we’ll run it on a batch of size 1 with our sentence. Remember, this is still untrained with random weights—just a mechanical demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model=4, num_heads=2):\n",
    "        \"\"\"\n",
    "        A simple multi-head self-attention for demonstration.\n",
    "        d_model = total embedding dimension,\n",
    "        num_heads = how many heads,\n",
    "        head_dim = d_model // num_heads.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        # Linear layers for Q, K, V\n",
    "        torch.manual_seed(123)\n",
    "        self.W_Q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_K = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_V = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "        # Final projection after concatenating heads\n",
    "        self.out_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: (batch_size, seq_len, d_model)\n",
    "        Returns: (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        bsz, seq_len, _ = x.size()\n",
    "\n",
    "        # 1) Compute Q, K, V => shape: (bsz, seq_len, d_model)\n",
    "        Q = self.W_Q(x)\n",
    "        K = self.W_K(x)\n",
    "        V = self.W_V(x)\n",
    "\n",
    "        # 2) Reshape for multi-head: => (bsz, seq_len, num_heads, head_dim) => transpose\n",
    "        # to (bsz, num_heads, seq_len, head_dim)\n",
    "        Q = Q.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        # Now shape is (bsz, num_heads, seq_len, head_dim)\n",
    "\n",
    "        # 3) Compute scaled dot-product attention for each head\n",
    "        #    scores => (bsz, num_heads, seq_len, seq_len)\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / (self.head_dim ** 0.5)\n",
    "        attn_weights = F.softmax(scores, dim=-1)  # across last dim => seq_len\n",
    "\n",
    "        # Weighted sum => context: (bsz, num_heads, seq_len, head_dim)\n",
    "        context = torch.matmul(attn_weights, V)\n",
    "\n",
    "        # 4) Transpose & reshape back to (bsz, seq_len, d_model)\n",
    "        context = context.transpose(1, 2).contiguous()  # (bsz, seq_len, num_heads, head_dim)\n",
    "        context = context.view(bsz, seq_len, self.d_model)\n",
    "\n",
    "        # 5) Final linear projection\n",
    "        out = self.out_proj(context)\n",
    "        return out, attn_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Explanation__\n",
    "\n",
    "- We create 𝑄,𝐾,𝑉 each of size `(batch_size, seq_len, d_model)`.\n",
    "- Reshape to `(batch_size, num_heads, seq_len, head_dim)`.\n",
    "- Dot-product attention per head → shape `(batch_size, num_heads, seq_len, head_dim)`.\n",
    "- Concat heads → `(batch_size, seq_len, d_model)`.\n",
    "- Final linear projection → `(batch_size, seq_len, d_model)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MHA Output shape: torch.Size([1, 8, 4])\n",
      "MHA Output: tensor([[[-0.1172,  0.0805, -0.3105,  0.2153],\n",
      "         [-0.1017,  0.0579, -0.3384,  0.1675],\n",
      "         [-0.1759,  0.1428, -0.3050,  0.3935],\n",
      "         [-0.1817,  0.1242, -0.3209,  0.4163],\n",
      "         [-0.0974,  0.0706, -0.2787,  0.1747],\n",
      "         [-0.1218,  0.0870, -0.2922,  0.2572],\n",
      "         [-0.1558,  0.1144, -0.3671,  0.3140],\n",
      "         [-0.0999,  0.0696, -0.2889,  0.1924]]], grad_fn=<UnsafeViewBackward0>)\n",
      "Attention Weights per head: tensor([[[[0.1174, 0.1157, 0.1151, 0.1445, 0.1276, 0.1423, 0.1021, 0.1353],\n",
      "          [0.1049, 0.1176, 0.0787, 0.1652, 0.1316, 0.1727, 0.0659, 0.1635],\n",
      "          [0.1124, 0.0697, 0.2446, 0.1225, 0.1006, 0.0949, 0.1778, 0.0776],\n",
      "          [0.1093, 0.0617, 0.2912, 0.0955, 0.0866, 0.0709, 0.2260, 0.0587],\n",
      "          [0.1327, 0.1206, 0.1679, 0.0930, 0.1109, 0.0895, 0.1923, 0.0932],\n",
      "          [0.1206, 0.0836, 0.2523, 0.0667, 0.0839, 0.0561, 0.2813, 0.0554],\n",
      "          [0.0907, 0.0821, 0.0897, 0.2099, 0.1253, 0.1932, 0.0540, 0.1550],\n",
      "          [0.1271, 0.0976, 0.2219, 0.0729, 0.0927, 0.0647, 0.2572, 0.0660]],\n",
      "\n",
      "         [[0.1150, 0.1014, 0.1743, 0.1390, 0.1029, 0.1088, 0.1576, 0.1009],\n",
      "          [0.1417, 0.1786, 0.0982, 0.1109, 0.1115, 0.0975, 0.1592, 0.1024],\n",
      "          [0.0739, 0.0452, 0.3435, 0.1514, 0.0527, 0.0658, 0.2175, 0.0500],\n",
      "          [0.1257, 0.1970, 0.1013, 0.0889, 0.0455, 0.0336, 0.3748, 0.0331],\n",
      "          [0.1108, 0.0923, 0.1642, 0.1381, 0.1189, 0.1311, 0.1214, 0.1232],\n",
      "          [0.1333, 0.1523, 0.1300, 0.1215, 0.0945, 0.0861, 0.1973, 0.0850],\n",
      "          [0.0932, 0.0691, 0.2649, 0.1485, 0.0672, 0.0762, 0.2177, 0.0630],\n",
      "          [0.1321, 0.1445, 0.1207, 0.1218, 0.1138, 0.1075, 0.1512, 0.1084]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Multi-head self-attention\n",
    "mha = MultiHeadSelfAttention(d_model=d_model, num_heads=2)\n",
    "mha_out, attn_weights_per_head = mha(embedded_tokens)\n",
    "\n",
    "print(\"MHA Output shape:\", mha_out.shape)  # (1, seq_len, 4)\n",
    "print(\"MHA Output:\", mha_out)\n",
    "print(\"Attention Weights per head:\", attn_weights_per_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ll see some random values. In a __trained__ scenario, each head would learn distinct patterns of attention (e.g., focusing on “shoes” ↔ “small”), but here it’s random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Key Takeaways__\n",
    "\n",
    "- __Split dimension__: We do `Q.view(...)` to split `d_model=4` into 2 heads, each dimension = 2.\n",
    "- __Parallel__: Both heads compute attention in parallel, then we concatenate.\n",
    "- __Final projection__: We ensure the output shape is `(batch_size, seq_len, d_model)` again.\n",
    "- __Untrained__: The random initialization means it won’t yield meaningful attention patterns—but it shows you the mechanics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add & Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1) Residuals__\n",
    "- We add the original input embeddings (`embedded_tokens`) back to the output of mha_out.\n",
    "- This helps the model retain original information if the MHA transform is partially or fully bypassed.\n",
    "- Also improves gradient flow in deep Transformers.\n",
    "\n",
    "__2) Layer Normalization__\n",
    "- Means each token embedding now has near zero mean and unit variance across its 4 features.\n",
    "- This stabilizes training and balances the scale of embeddings for subsequent layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After Add & Norm:\n",
      "Residual shape: torch.Size([1, 8, 4])\n",
      "Normalized output shape: torch.Size([1, 8, 4])\n",
      "Normalized output: tensor([[[ 1.5543,  0.2013, -0.8427, -0.9129],\n",
      "         [ 0.5031,  1.3901, -1.0524, -0.8408],\n",
      "         [ 0.7244, -0.4937,  1.1506, -1.3812],\n",
      "         [ 0.6876,  0.6453,  0.3876, -1.7206],\n",
      "         [ 0.7042, -1.2470, -0.6777,  1.2205],\n",
      "         [ 0.4706, -1.6513,  0.1694,  1.0113],\n",
      "         [ 0.8681,  0.4527,  0.3810, -1.7018],\n",
      "         [-0.6900, -1.1166,  0.3356,  1.4711]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# ADD & NORM LAYER\n",
    "# ================================\n",
    "\n",
    "# 1) Residual\n",
    "residual_1 = embedded_tokens + mha_out\n",
    "\n",
    "# 2) Layer Normalization\n",
    "layer_norm_1 = nn.LayerNorm(normalized_shape=d_model)\n",
    "normed_1 = layer_norm_1(residual_1)\n",
    "\n",
    "print(\"\\nAfter Add & Norm:\")\n",
    "print(\"Residual shape:\", residual_1.shape)      # [1, seq_len, 4]\n",
    "print(\"Normalized output shape:\", normed_1.shape)  # [1, seq_len, 4]\n",
    "print(\"Normalized output:\", normed_1)  # [1, seq_len, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed Forward Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Processes each token independently with two linear layers + ReLU.\n",
    "- Goes from `d_model=4` → `hidden_dim=8` → back to 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model=4, hidden_dim=8):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_dim, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "        out = self.linear1(x)   # expand\n",
    "        out = self.relu(out)    # non-linear\n",
    "        out = self.linear2(out) # back to d_model\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1996, -0.7154, -0.0501, -0.1783],\n",
      "         [ 0.1308, -0.4855,  0.0238, -0.0576],\n",
      "         [ 0.2613, -0.3266,  0.0152, -0.0999],\n",
      "         [ 0.3176, -0.4831, -0.0302, -0.1423],\n",
      "         [-0.0124, -0.4780,  0.0971, -0.3309],\n",
      "         [ 0.0464, -0.3165,  0.1180, -0.3305],\n",
      "         [ 0.3129, -0.4818, -0.0272, -0.1422],\n",
      "         [ 0.0503, -0.1795,  0.1856, -0.3700]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForward(d_model=d_model, hidden_dim=8)\n",
    "ffn_out = ffn(normed_1) # (1, seq_len, 4)\n",
    "print(ffn_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- After Feed-Forward --\n",
      "FFN output shape: torch.Size([1, 8, 2])\n",
      "Final encoder output shape: torch.Size([1, 8, 2])\n",
      "Final encoder output: tensor([[[-1.0000,  1.0000],\n",
      "         [-1.0000,  1.0000],\n",
      "         [-1.0000,  1.0000],\n",
      "         [-1.0000,  1.0000],\n",
      "         [ 1.0000, -1.0000],\n",
      "         [ 1.0000, -1.0000],\n",
      "         [-1.0000,  1.0000],\n",
      "         [-1.0000,  1.0000]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "residual_2 = normed_1 + ffn_out\n",
    "layer_norm_2 = nn.LayerNorm(normalized_shape=2)\n",
    "encoder_out = layer_norm_2(residual_2)\n",
    "\n",
    "print(\"\\n-- After Feed-Forward --\")\n",
    "print(\"FFN output shape:\", ffn_out.shape)\n",
    "print(\"Final encoder output shape:\", encoder_out.shape)\n",
    "print(\"Final encoder output:\", encoder_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a full Transformer encoder, you’d stack multiple such blocks and potentially include positional embeddings at the start. However, this snippet shows the core steps for computing the FFN after MHA, along with the residual + layer normalization surrounding each sublayer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add & Norm (again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- After Feed-Forward --\n",
      "FFN output shape: torch.Size([1, 8, 4])\n",
      "Final encoder output shape: torch.Size([1, 8, 4])\n",
      "Final encoder output: tensor([[[ 1.7031, -0.2881, -0.6204, -0.7946],\n",
      "         [ 0.8375,  1.1476, -1.0672, -0.9180],\n",
      "         [ 0.8981, -0.6871,  1.0562, -1.2672],\n",
      "         [ 1.0155,  0.2299,  0.4118, -1.6572],\n",
      "         [ 0.8274, -1.4635, -0.3788,  1.0149],\n",
      "         [ 0.5928, -1.7174,  0.3793,  0.7452],\n",
      "         [ 1.1443,  0.0501,  0.3964, -1.5908],\n",
      "         [-0.5959, -1.2929,  0.6365,  1.2523]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 1) Residual Connection\n",
    "# Add the FFN output back to the already normalized output of the MHA block\n",
    "residual_2 = normed_1 + ffn_out\n",
    "\n",
    "# 2) Layer Normalization\n",
    "# Typically the same dimension as 'd_model'\n",
    "d_model = 4  \n",
    "layer_norm_2 = nn.LayerNorm(normalized_shape=d_model)\n",
    "encoder_out = layer_norm_2(residual_2)\n",
    "\n",
    "print(\"-- After Feed-Forward --\")\n",
    "print(\"FFN output shape:\", ffn_out.shape)           # e.g. [1, seq_len, 4]\n",
    "print(\"Final encoder output shape:\", encoder_out.shape)  # [1, seq_len, 4]\n",
    "print(\"Final encoder output:\", encoder_out)  # [1, seq_len, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. __Residual__\n",
    "\n",
    "This ensures the original, already-normalized MHA information (`normed_1`) is kept, in case the FFN output is unhelpful or partially helpful for certain tokens.\n",
    "\n",
    "2. __Layer Norm__\n",
    "- Normalizes along the embedding dimension (e.g., 4 in our toy example).\n",
    "- Ensures per-token embeddings have zero mean and unit variance, stabilizing training and balancing contributions from the residual and the FFN."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
