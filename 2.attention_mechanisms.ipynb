{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/project_stage_attn.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/11_attention_mechanism_types.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.5.1\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "print(\"torch version:\", version(\"torch\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before the Transformer models, machine learning translation happened using RNNs. \n",
    "\n",
    "\n",
    "The key idea here is that the encoder part processes the entire input text into a hidden state (memory cell). The decoder then takes in this hidden state to produce the output. You can think of this hidden state as an embedding vector,\n",
    "\n",
    "<img src=\"./images/11_rnn_enc_dec.png\" width=\"400px\">\n",
    "\n",
    "The big issue and limitation of encoder-decoder RNNs is that the RNN can't directly access earlier hidden states from the encoder during the decoding phase. Consequently, it relies solely on the current hidden state, which encapsulates all relevant information. This can lead to a loss of context, especially in complex sentences where dependencies might span long distances.\n",
    "\n",
    "Hence, researchers developed the so-called Bahdanau <b> attention </b> mechanism for RNNs in 2014 (named after the first author of the respective paper), which modifies the encoder-decoder RNN such that the decoder can selectively access different parts of the input sequence at each decoding step.\n",
    "\n",
    "Using an attention mechanism, the text-generating decoder part of the network can access all input tokens selectively. This means that some input tokens are more important than others for generating a given output token. The importance is determined by the so-called\n",
    "attention weights, which we will discuss later. Note that this figure shows the general idea behind attention and does not depict the exact implementation of the Bahdanau mechanism, which is an RNN method.\n",
    "\n",
    "<img src=\"./images/11_rnn_bahdanau.png\" width=\"400px\">\n",
    "\n",
    "Later, researchers found that RNN architectures are not required for building deep neural networks for natural language\n",
    "processing and proposed the original <i> transformer </i> architecture with a self-attention mechanism inspired by the Bahdanau\n",
    "attention mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention: Attending to different parts of the input with self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Difference between Seq2Seq and Attention </b>\n",
    "\n",
    "- In self-attention, the \"self\" refers to the mechanism's ability to compute attention weights by relating different positions within a single input sequence. It assesses and learns the relationships and dependencies between various parts of the input itself, such as words in a sentence or pixels in an image. \n",
    "- This is in contrast to traditional attention mechanisms, where the focus is on the relationships between elements of two different sequences, such as in sequence-to-sequence models where the attention might be between an <b> input </b> sequence and an <b>output</b> sequence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Self-Attention (No Trainable Weights)\n",
    "\n",
    "we implement a simplified variant of self-attention, free from any trainable weights. This is to illustrate some key concepts before adding trainable weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Suppose we are given an input sequence $x^{(1)}$ to $x^{(T)}$\n",
    "  - The input is a text (for example, a sentence like \"Your journey starts with one step\") that has already been converted into token embeddings as described in <a href=\"./1.working_with_text.ipynb\">./1.working_with_text.ipynb</a>\n",
    "  - For instance, $x^{(1)}$ is a d-dimensional vector representing the word \"Your\", and so forth\n",
    "- **Goal:** compute context vectors $z^{(i)}$ for each input sequence element $x^{(i)}$ in $x^{(1)}$ to $x^{(T)}$ (where $z$ and $x$ have the same dimension)\n",
    "    - A context vector $z^{(i)}$ is a weighted sum over the inputs $x^{(1)}$ to $x^{(T)}$\n",
    "    - The context vector is \"context\"-specific to a certain input\n",
    "      - Instead of $x^{(i)}$ as a placeholder for an arbitrary input token, let's consider the second input, $x^{(2)}$\n",
    "      - And to continue with a concrete example, instead of the placeholder $z^{(i)}$, we consider the second output context vector, $z^{(2)}$\n",
    "      - The second context vector, $z^{(2)}$, is a weighted sum over all inputs $x^{(1)}$ to $x^{(T)}$ weighted with respect to the second input element, $x^{(2)}$\n",
    "      - The attention weights are the weights that determine how much each of the input elements contributes to the weighted sum when computing $z^{(2)}$\n",
    "      - In short, think of $z^{(2)}$ as a modified version of $x^{(2)}$ that also incorporates information about all other input elements that are relevant to a given task at hand\n",
    "\n",
    "<img src=\"./images/11_attention_simplified_01.png\" width=\"400px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following input sentence, which has already been embedded into 3-dimensional vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (We follow the common machine learning and deep learning convention where training examples are represented as rows and feature values as columns; in the case of the tensor shown above, each row represents a word, and each column represents an embedding dimension)\n",
    "\n",
    "- The first step of implementing self-attention is to compute the intermediate values ω, referred to as <b> attention scores </b>\n",
    "  - By convention, the unnormalized attention weights are referred to as **\"attention scores\"** whereas the normalized attention scores, which sum to 1, are referred to as **\"attention weights\"**\n",
    "\n",
    "- The primary objective of this section is to demonstrate how the context vector $z^{(2)}$ is calculated using the second input sequence, $x^{(2)}$, as a query\n",
    "\n",
    "- The figure depicts the initial step in this process, which involves calculating the attention scores ω between $x^{(2)}$\n",
    "  and all other input elements through a dot product operation\n",
    "\n",
    "<img src=\"./images/11_attn_scores.png\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first step is to compute the unnormalized attention scores by computing the dot product between the query $x^{(2)}$ and all other input tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "# Calculate the intermediate attention scores between the query token (x ^ 2) and each input token. \n",
    "# We determine these scores by computing the dot product of the query, x(2), with every other input token:\n",
    "\n",
    "query = inputs[1]  # 2nd input token is the query\n",
    "\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query) # dot product (transpose not necessary here since they are 1-dim vectors)\n",
    "\n",
    "# Atention score for the second input token\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "- **Step 1:** compute unnormalized attention scores $\\omega$\n",
    "- Suppose we use the second input token as the query, that is, $q^{(2)} = x^{(2)}$, we compute the unnormalized attention scores via dot products:\n",
    "    - $\\omega_{21} = x^{(1)} q^{(2)\\top}$\n",
    "    - $\\omega_{22} = x^{(2)} q^{(2)\\top}$\n",
    "    - $\\omega_{23} = x^{(3)} q^{(2)\\top}$\n",
    "    - ...\n",
    "    - $\\omega_{2T} = x^{(T)} q^{(2)\\top}$\n",
    "- Above, $\\omega$ is the Greek letter \"omega\" used to symbolize the unnormalized attention scores\n",
    "    - The subscript \"21\" in $\\omega_{21}$ means that input sequence element 2 was used as a query against input sequence element 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Side note: a dot product is essentially a shorthand for multiplying two vectors elements-wise and summing the resulting products:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " tensor([0.4300, 0.1500, 0.8900])\n",
      "Query: \n",
      " tensor([0.5500, 0.8700, 0.6600]) \n",
      "\n",
      "Multiplying query and input array in a normal way \n",
      "\n",
      "0.4300000071525574 * 0.550000011920929 = 0.23650000989437103\n",
      "0.15000000596046448 * 0.8700000047683716 = 0.13050000369548798\n",
      "0.8899999856948853 * 0.6600000262260437 = 0.5874000191688538\n",
      "tensor(0.9544)\n",
      "\n",
      " Using torch dot product: \n",
      " 0.9544000625610352\n"
     ]
    }
   ],
   "source": [
    "res = 0.\n",
    "\n",
    "print(f\"Input: \\n {inputs[0]}\")\n",
    "print(f\"Query: \\n {query} \\n\")\n",
    "\n",
    "print(\"Multiplying query and input array in a normal way \\n\")\n",
    "for idx, element in enumerate(inputs[0]):\n",
    "    print (f\"{inputs[0][idx]} * { query[idx]} = {inputs[0][idx] * query[idx]}\")\n",
    "    res += inputs[0][idx] * query[idx]\n",
    "\n",
    "print(res)\n",
    "print(f\"\\n Using torch dot product: \\n {torch.dot(inputs[0], query)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Step 2:** normalize the unnormalized attention scores (\"omegas\", $\\omega$) so that they sum up to 1\n",
    "- Here is a simple way to normalize the unnormalized attention scores to sum up to 1 (a convention, useful for interpretation, and important for training stability):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/11_attention_wts.png\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum: tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "# Here's a straightforward method for achieving this normalization step:\n",
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2_tmp)\n",
    "print(\"Sum:\", attn_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- However, in practice, using the softmax function for normalization, which is better at handling extreme values and has more desirable gradient properties during training, is common and recommended.\n",
    "- Here's a naive implementation of a softmax function for scaling, which also normalizes the vector elements such that they sum up to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2_naive)\n",
    "print(\"Sum:\", attn_weights_2_naive.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The naive implementation above can suffer from numerical instability issues for large or small input values due to overflow and underflow issues\n",
    "- Hence, in practice, it's recommended to use the PyTorch implementation of softmax instead, which has been highly optimized for performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Step 3**: compute the context vector $z^{(2)}$ by multiplying the embedded input tokens, $x^{(i)}$ with the attention weights and sum the resulting vectors. This context vector is a combination of all input vectors  $x^{(1)}$ to  $x^{(T)}$ weighted by the attention weights.\n",
    "\n",
    "<img src=\"./images/11_context_vector.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs \n",
      " tensor([[0.4300, 0.1500, 0.8900],\n",
      "        [0.5500, 0.8700, 0.6600],\n",
      "        [0.5700, 0.8500, 0.6400],\n",
      "        [0.2200, 0.5800, 0.3300],\n",
      "        [0.7700, 0.2500, 0.1000],\n",
      "        [0.0500, 0.8000, 0.5500]])\n",
      "Attn Weights \n",
      " tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "attn_weights_2[0]: 0.13854756951332092 * Input: tensor([0.4300, 0.1500, 0.8900])  = tensor([0.0596, 0.0208, 0.1233])\n",
      "attn_weights_2[1]: 0.2378913015127182 * Input: tensor([0.5500, 0.8700, 0.6600])  = tensor([0.1308, 0.2070, 0.1570])\n",
      "attn_weights_2[2]: 0.23327402770519257 * Input: tensor([0.5700, 0.8500, 0.6400])  = tensor([0.1330, 0.1983, 0.1493])\n",
      "attn_weights_2[3]: 0.12399158626794815 * Input: tensor([0.2200, 0.5800, 0.3300])  = tensor([0.0273, 0.0719, 0.0409])\n",
      "attn_weights_2[4]: 0.10818186402320862 * Input: tensor([0.7700, 0.2500, 0.1000])  = tensor([0.0833, 0.0270, 0.0108])\n",
      "attn_weights_2[5]: 0.15811361372470856 * Input: tensor([0.0500, 0.8000, 0.5500])  = tensor([0.0079, 0.1265, 0.0870])\n",
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1] # 2nd input token is the query\n",
    "\n",
    "print(\"Inputs \\n\", inputs)\n",
    "print(\"Attn Weights \\n\", attn_weights_2)\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i,x_i in enumerate(inputs):\n",
    "    print(f\"attn_weights_2[{i}]: {attn_weights_2[i]} * Input: {x_i}  = {attn_weights_2[i]*x_i}\")\n",
    "    context_vec_2 += attn_weights_2[i]*x_i\n",
    "\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, this is how we constructed the context vector:\n",
    "\n",
    "<img src=\"./images/11_context_vector_01.png\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets Compute Context Vectors for all input tokens**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Above, we computed the attention weights and context vector for input 2 (as illustrated in the highlighted row in the figure below)\n",
    "- Next, we are generalizing this computation to compute all attention weights and context vectors.\n",
    "\n",
    "    - (Please note that the numbers in this figure are truncated to two\n",
    "digits after the decimal point to reduce visual clutter; the values in each row should add up to 1.0 or 100%; similarly, digits in other figures are truncated)\n",
    "\n",
    "<img src=\"./images/11_context_vector_2.png\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to follow the same 3 steps as before, while making few modifications in the code to compute all context\n",
    "vectors instead of only the second.\n",
    "\n",
    "<img src=\"./images/11_context_vector_all.png\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Apply previous **step 1** to all pairwise elements to compute the unnormalized attention score matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.empty(6, 6)\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can achieve the same as above more efficiently via matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = inputs @ inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Similar to **step 2** previously, we normalize each row so that the values in each row sum to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Quick verification that the values in each row indeed sum to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 2 sum: 1.0\n",
      "All row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "row_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
    "print(\"Row 2 sum:\", row_2_sum)\n",
    "\n",
    "print(\"All row sums:\", attn_weights.sum(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Apply previous **step 3** to compute all context vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "all_context_vecs = attn_weights @ inputs\n",
    "print(all_context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As a sanity check, the previously computed context vector $z^{(2)} = [0.4419, 0.6515, 0.5683]$ can be found in the 2nd row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous 2nd context vector: tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "print(\"Previous 2nd context vector:\", context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Self-Attention (With Trainable Weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are implementing the self-attention mechanism that is used in the original transformer architecture, the GPT models, and most other popular LLMs. This self-attention mechanism is also called **scaled dot-product attention**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/12_self-attention_goal.png\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This self-attention mechanism is also called \"scaled dot-product attention\"\n",
    "- The overall idea is similar to before:\n",
    "  - We want to compute context vectors as weighted sums over the input vectors specific to a certain input element\n",
    "  - For the above, we need attention weights\n",
    "- As you will see, there are only slight differences compared to the basic attention mechanism introduced earlier:\n",
    "  - The most notable difference is the introduction of **weight** matrices that are updated during model training\n",
    "  - These trainable weight matrices are crucial so that the model (specifically, the attention module inside the model) can learn to produce \"good\" context vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first step of the self-attention mechanism with trainable weight matrices, we compute query ($q$), key ($k$), and value ($v$) vectors for input elements x. Similar to previous sections, we designate the second input, $x^{(2)}$, as the query input. Actually, the query vector $q^{(2)}$ is obtained via matrix multiplication between the input $x^{(2)}$ and the weight matrix $W_q$. Similarly, we obtain the key and value vectors via matrix multiplication involving the weight matrices $W_k$ and $W_v$.\n",
    "\n",
    "<img src=\"./images/12_weight_matrices.png\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implementing the self-attention mechanism step by step, we will start by introducing the three training weight matrices $W_q$, $W_k$, and $W_v$\n",
    "- These three matrices are used to project the embedded input tokens, $x^{(i)}$, into query, key, and value vectors via matrix multiplication:\n",
    "\n",
    "  - Query vector: $q^{(i)} = W_q \\,x^{(i)}$\n",
    "  - Key vector: $k^{(i)} = W_k \\,x^{(i)}$\n",
    "  - Value vector: $v^{(i)} = W_v \\,x^{(i)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The embedding dimensions of the input $x$ and the query vector $q$ can be the same or different, depending on the model's design and specific implementation\n",
    "- In GPT models, the input and output dimensions are usually the same, but for illustration purposes, to better follow the computation, we choose different input and output dimensions here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1] # second input element\n",
    "d_in = inputs.shape[1] # the input embedding size, d=3\n",
    "d_out = 2 # the output embedding size, d=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Below, we initialize the three weight matrices; note that we are setting `requires_grad=False` to reduce clutter in the outputs for illustration purposes, but if we were to use the weight matrices for model training, we would set `requires_grad=True` to update these matrices during model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next we compute the query, key, and value vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query # _2 because it's with respect to the 2nd input element\n",
    "key_2 = x_2 @ W_key \n",
    "value_2 = x_2 @ W_value\n",
    "\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we can see below, we successfully projected the 6 input tokens from a 3D onto a 2D embedding space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query.shape: torch.Size([2])\n",
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key \n",
    "values = inputs @ W_value\n",
    "\n",
    "print(\"query.shape:\", query_2.shape)\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the next step, **step 2**, we compute the unnormalized attention scores by computing the dot product between the query and each key vector:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/12_attn_score_key_query.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys for all your inputs: \n",
      " tensor([[0.3669, 0.7646],\n",
      "        [0.4433, 1.1419],\n",
      "        [0.4361, 1.1156],\n",
      "        [0.2408, 0.6706],\n",
      "        [0.1827, 0.3292],\n",
      "        [0.3275, 0.9642]]) \n",
      "\n",
      "Query for input 2: \n",
      " tensor([0.4306, 1.4551]) \n",
      "\n",
      "W22 Attention score - tensor([0.4306, 1.4551]) * tensor([0.4433, 1.1419]) = 1.8523844480514526\n"
     ]
    }
   ],
   "source": [
    "print(f\"Keys for all your inputs: \\n {keys} \\n\")\n",
    "print(f\"Query for input 2: \\n {query_2} \\n\")\n",
    "# get the second key index\n",
    "keys_2 = keys[1] # Python starts index at 0\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(f\"W22 Attention score - {query_2} * {keys_2} = {attn_score_22}\") # calculating W22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since we have 6 inputs, we have 6 attention scores for the given query vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2 = query_2 @ keys.T # All attention scores for given query\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next, in **step 3**, we compute the attention weights α (normalized attention scores that sum up to 1) using the softmax function we used earlier\n",
    "- The difference to earlier is that we now scale the attention scores by dividing them by the square root of the embedding dimension, $\\sqrt{d_k}$ (i.e., `d_k**0.5`):\n",
    "- The reason for the normalization by the embedding dimension size is to improve the training performance by avoiding small gradients. This is especially true when you are sclaing up embedding dimension for GPT-like LLMs, which is usually in thousands.\n",
    "- The scaling by the square root of the embedding dimension is the reason why this self-attention mechanism is also called scaled-dot product attention.\n",
    "\n",
    "<img src=\"./images/12_attn_score.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In **step 4**, we now compute the context vector for input query vector 2:\n",
    "    - In the final step of the self-attention computation, we compute the context vector by combining all the **Value** vectors via the attention weights. $v^{(i)} α^{(2i)}$\n",
    "\n",
    "<img src=\"./images/12_attn_wt_value_vector.png\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we only computed a single context vector, $Z^{(2)}$.\n",
    "\n",
    "We will generalize the code to compute all context vectors in the input sequence,$Z^{(1)}$ to $Z^{(T)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Putting it all together, we can implement the self-attention mechanism as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this PyTorch code, SelfAttention_v1 is a class derived from `nn.Module`, which is a fundamental building block of PyTorch models, which provides necessary functionalities for model layer creation and management.\n",
    "- The __init__ method initializes trainable weight matrices (W_query, W_key, and W_value) for queries, keys, and values, each transforming the input dimension d_in to an output dimension d_out.\n",
    "- During the forward pass, using the forward method, we compute the attention scores (`attn_scores`) by multiplying queries and keys, normalizing these scores using softmax. Finally, we create a context vector by weighting the values with these normalized attention scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        \n",
    "        attn_scores = queries @ keys.T # omega\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In self-attention, we transform the input vectors in the input matrix X with the three weight matrices, Wq, Wk, and Wv. Then, we compute the attention weight matrix based on the resulting queries (Q) and keys (K). \n",
    "\n",
    "Using the attention weights and values (V), we then compute the context vectors (Z). \n",
    "\n",
    "(For visual clarity, we focus on a single input text with n tokens in this figure, not a batch of multiple inputs. Consequently, the 3D input tensor is simplified to a 2D\n",
    "matrix in this context.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/12_self_attn_values.png\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can improve the `SelfAttention_v1` implementation further by utilizing PyTorch's `nn.Linear` layers, which effectively perform matrix multiplication when the bias units are disabled. Additionally, a significant advantage of using `nn.Linear` instead of manually implementing `nn.Parameter(torch.rand(...)) `is that `nn.Linear` has an optimized weight initialization scheme, contributing to more stable and effective model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that `SelfAttention_v1` and `SelfAttention_v2` give different outputs because they use different initial weights for the weight matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUERY, KEY & VALUES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The terms \"key,\" \"query,\" and \"value\" in the context of attention mechanisms are borrowed from the domain of information retrieval and databases, where similar concepts are used to store, search, and retrieve information.\n",
    "\n",
    "\n",
    "A \"query\" is analogous to a search query in a database. It represents the current item (e.g., a word or token in a sentence) the model focuses on or tries to understand. The query is used to probe the other parts of the input sequence to determine how much attention to pay to them.\n",
    "\n",
    "\n",
    "The \"key\" is like a database key used for indexing and searching. In the attention mechanism, each item in the input sequence (e.g., each word in a sentence) has an associated key. These keys are used to match with the query.\n",
    "\n",
    "\n",
    "The \"value\" in this context is similar to the value in a key-value pair in a database. It represents the actual content or representation of the input items. Once the model determines which keys (and thus which parts of the input) are most relevant to the query (the current focus item), it retrieves the corresponding values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Causal attention**, also known as *masked attention*, is a specialized form of self-attention. It restricts a model to only consider previous and current inputs in a sequence when processing any given token. This is in contrast to the standard self-attention mechanism, which allows access to the entire input sequence at once.\n",
    "\n",
    "- The causal aspect involves modifying the attention mechanism to prevent the model from accessing future information in the sequence, which is crucial for tasks like language modeling, where each word prediction should only depend on previous words.\n",
    "\n",
    "- To achieve this in GPT-like LLMs, for each token processed, we mask out the future tokens, which come after the current token in the input text\n",
    "\n",
    "- In causal attention, the attention weights above the diagonal are masked, ensuring that for any given input, the LLM is unable to utilize future tokens while calculating the context vectors with the attention weight\n",
    "\n",
    "<img src=\"./images/12_causal_attn.png\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One way to obtain the masked attention weight matrix in causal attention is to apply the softmax function to the attention scores, zeroing out the elements above the diagonal and normalizing the resulting matrix.\n",
    "\n",
    "<img src=\"./images/12_causal_attn_apply.png\" width=\"500px\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To illustrate and implement causal self-attention, let's work with the attention scores and weights from the previous section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Reuse the query and key weight matrices of the\n",
    "# SelfAttention_v2 object from the previous section for convenience\n",
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs) \n",
    "attn_scores = queries @ keys.T\n",
    "\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The simplest way to mask out future attention weights is by creating a mask via PyTorch's `torch.tril` function with elements below the main diagonal (including the diagonal itself) set to 1 and above the main diagonal set to 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Then, we can multiply the attention weights with this mask to zero out the attention scores above the diagonal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_simple = attn_weights*mask_simple\n",
    "print(masked_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- However, if the mask were applied after softmax, like above, it would disrupt the probability distribution created by softmax\n",
    "- Softmax ensures that all output values sum to 1\n",
    "- Masking after softmax would require re-normalizing the outputs to sum to 1 again, which complicates the process and might lead to unintended effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To make sure that the rows sum to 1, we can normalize the attention weights as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "row_sums = masked_simple.sum(dim=-1, keepdim=True)\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(masked_simple_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- While we are technically done with coding the causal attention mechanism now, let's briefly look at a more efficient approach to achieve the same as above\n",
    "- So, instead of zeroing out attention weights above the diagonal and renormalizing the results, we can mask the unnormalized attention scores above the diagonal with negative infinity before they enter the softmax function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/12_causal_attn_inf.png\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we can see below, now the attention weights in each row correctly sum to 1 again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we cover another minor tweak to the causal attention mechanism that is useful for reducing overfitting when training LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking additional attention weights with dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In addition, we also apply dropout to reduce overfitting during training\n",
    "- Dropout can be applied in several places:\n",
    "  - for example, after computing the attention weights;\n",
    "  - or after multiplying the attention weights with the value vectors\n",
    "- Here, we will apply the dropout mask after computing the attention weights because it's more common\n",
    "\n",
    "- Furthermore, in this specific example, we use a dropout rate of 50%, which means randomly masking out half of the attention weights. (When we train the GPT model later, we will use a lower dropout rate, such as 0.1 or 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the causal attention mask (upper left), we apply an additional dropout mask (upper right) to zero out additional attention weights to reduce overfitting during training.\n",
    "\n",
    "<img src=\"./images/12_causal_attn_dropout.png\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When applying dropout to an attention weight matrix with a rate of 50%, half of the elements in the matrix are randomly set to zero. To compensate for the reduction in active elements, the values of the remaining elements in the matrix are scaled up by a factor of 1/0.5 =2. This scaling is crucial to maintain the overall balance of the attention weights, ensuring that the average influence of the attention mechanism remains consistent during both the training and inference phases.\n",
    "- The scaling is calculated by the formula 1 / (1 - `dropout_rate`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2., 2., 2., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.],\n",
      "        [0., 0., 2., 0., 2., 0.],\n",
      "        [2., 2., 0., 0., 0., 2.],\n",
      "        [2., 0., 0., 0., 0., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# In the following code, we apply PyTorch's dropout implementation first to a 6×6 tensor consisting of ones\n",
    "\n",
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5) # dropout rate of 50%\n",
    "example = torch.ones(6, 6) # create a matrix of ones\n",
    "\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we can see above, approximately half of the values are zeroed out.\n",
    "- Now, let's apply dropout to the attention weight matrix itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.8966, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4921, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4350, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3327, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a compact causal self-attention class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, we are ready to implement a working implementation of self-attention, including the causal and dropout masks\n",
    "- One more thing is to implement the code to handle batches consisting of more than one input so that our `CausalAttention` class supports the batch outputs produced by the data loader.\n",
    "- For simplicity, to simulate such batch input, we duplicate the input text example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4300, 0.1500, 0.8900],\n",
       "        [0.5500, 0.8700, 0.6600],\n",
       "        [0.5700, 0.8500, 0.6400],\n",
       "        [0.2200, 0.5800, 0.3300],\n",
       "        [0.7700, 0.2500, 0.1000],\n",
       "        [0.0500, 0.8000, 0.5500]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, to simulate such batch inputs, we duplicate the input text example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n",
      "tensor([[[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]],\n",
      "\n",
      "        [[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]]])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape) # 2 inputs with 6 tokens each, and each token has embedding dimension 3\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The above results in a 3D tensor consisting of 2 input texts with 6 tokens each, where each token is a 3-dimensional embedding vector:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The following `CausalAttention` class is similar to the `SelfAttention` class we implemented earlier, except that we now added the dropout and causal mask components as highlighted in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]],\n",
      "\n",
      "        [[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length,\n",
    "                 dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout) # New\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) # New\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape # New batch dimension b\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1, 2) # Changed transpose\n",
    "        attn_scores.masked_fill_(  # New, _ ops are in-place\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)  # `:num_tokens` to account for cases where the number of tokens in the batch is smaller than the supported context_size\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        attn_weights = self.dropout(attn_weights) # New\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "\n",
    "context_vecs = ca(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The resulting context vector is a 3D tensor where each token is now represented by a 2D embedding: `context_vecs.shape: torch.Size([2, 6, 2])`\n",
    "\n",
    "- While all added code lines should be familiar from previous sections, we now added a self.register_buffer() call in the __init__ method. The use of `register_buffer` in PyTorch is not strictly necessary for all use cases but offers several advantages here. For instance, when we use the CausalAttention class in our LLM, buffers are automatically moved to the appropriate device (CPU or GPU) along with our model, which will be relevant when training the LLM in future chapters. This means we don't need to manually ensure these tensors are on the same device as your model parameters, avoiding device mismatch errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next, we will expand on this concept and implement a multi-head attention module that implements several of such causal attention mechanisms in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extending single-head attention to multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term **\"multi-head\"** refers to dividing the attention mechanism into multiple \"heads\", each operating independently. In this context, a single causal attention module can be considered single-head attention, where there is only one set of attention weights processing the input sequentially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first subsection will intuitively build a multi-head attention module by stacking multiple CausalAttention modules for illustration purposes. The second subsection will then implement the same multi-head attention module in a more complicated but computationally more efficient way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacking multiple single-head attention layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it is important to note that all tensors must be on the same device. Otherwise, the computation will fail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multi-head attention module in this figure depicts two single-head attention modules stacked on top of each other. \n",
    "\n",
    "So, instead of using a single matrix $W_v$ for computing the value matrices, in a multi-head attention module with two heads, we now have two \"Value\" weight matrices: ***W***<sub>v1</sub> and ***W***<sub>v2</sub>. The same applies to the other weight matrices, $W_q$ and $W_k$. We obtain two sets of context vectors $Z_1$ and $Z_2$ that we can combine into a single context vector matrix $Z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/12_mutihead_stacking.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The main idea behind multi-head attention is to run the attention mechanism multiple times (in parallel) with different, learned linear projections. This allows the model to jointly attend to information from different representation subspaces at different positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In code, we can achieve this by implementing a simple `MultiHeadAttentionWrapper` class that stacks multiple instances of our previously implemented `CausalAttention` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) \n",
    "             for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "context_length = batch.shape[1] # This is the number of tokens\n",
    "d_in, d_out = 3, 2\n",
    "mha = MultiHeadAttentionWrapper(\n",
    "    d_in, d_out, context_length, 0.0, num_heads=2\n",
    ")\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above result, the first dimension of the resulting `context_vecs` tensor is 2 since we have two input texts (the input texts are duplicated, which is why the context vectors are exactly the same for those). The second dimension refers to the 6 tokens in each input. The third dimension refers to the 4-dimensional embedding of each token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if we use this `MultiHeadAttentionWrapper` class with two attention heads (via num_heads=2) and CausalAttention output dimension d_out=2, this results in a 4-dimensional context vectors (d_out*num_heads=4),\n",
    "\n",
    "<img src=\"./images/12_multihead_context_vec.png\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we implemented a `MultiHeadAttentionWrapper` that combined multiple single-head attention modules. However, note that these are processed sequentially via `[head(x) for head in self.heads]` in the forward method. We can improve this implementation by processing the heads in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing multi-head attention with weight splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- While the above is an intuitive and fully functional implementation of multi-head attention (wrapping the single-head attention `CausalAttention` implementation from earlier), we can write a stand-alone class called `MultiHeadAttention` to achieve the same\n",
    "    - Instead of maintaining two separate classes, `MultiHeadAttentionWrapper` and `CausalAttention`, we can combine both of these concepts into a single `MultiHeadAttention` class. Also, in addition to just merging the `MultiHeadAttentionWrapper` with the `CausalAttention` code, we will make some other modifications to implement multi-head attention more efficiently.\n",
    "\n",
    "- In the previous `MultiHeadAttentionWrapper`, multiple heads are implemented by creating a list of `CausalAttention` objects (`self.heads`), each representing a separate attention head. The `CausalAttention` class independently performs the attention mechanism, and the results from each head are concatenated. In contrast, the following `MultiHeadAttention` class integrates the multi-head\n",
    "functionality within a single class. It splits the input into multiple heads by reshaping the projected query, key, and value tensors and then combines the results from these heads after computing attention.\n",
    "    - We don't concatenate single attention heads for this stand-alone `MultiHeadAttention` class. Instead, we create single W_query, W_key, and W_value weight matrices and then split those into individual matrices for each attention head:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " batch: tensor([[[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]],\n",
      "\n",
      "        [[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]]])\n",
      "\n",
      " batch shape: torch.Size([2, 6, 3])\n",
      "\n",
      " batch size: 2\n",
      "\n",
      " context_length: 6\n",
      "\n",
      " d_in: 3\n",
      "\n",
      " d_out: 2\n",
      "\n",
      " Calling MultiHeadAttention(d_in:3, d_out:2, context_length:6, dropout:0.0, num_heads=2)\n",
      "\n",
      " Init: \n",
      " MultiHeadAttention(\n",
      "  (W_query): Linear(in_features=3, out_features=2, bias=False)\n",
      "  (W_key): Linear(in_features=3, out_features=2, bias=False)\n",
      "  (W_value): Linear(in_features=3, out_features=2, bias=False)\n",
      "  (out_proj): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Query weight: \n",
      " Parameter containing:\n",
      "tensor([[-0.2354,  0.0191, -0.2867],\n",
      "        [ 0.2177, -0.4919,  0.4232]], requires_grad=True)\n",
      "[forward]: input tensor \n",
      " tensor([[[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]],\n",
      "\n",
      "        [[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]]])\n",
      "\n",
      " input shape (b, num_tokesn,d_in):  torch.Size([2, 6, 3])\n",
      "Keys, Queries and Values - BEFORE SPLITTING \n",
      "\n",
      "\n",
      " Keys \n",
      " tensor([[[-0.5740,  0.2727],\n",
      "         [-0.8709,  0.1008],\n",
      "         [-0.8628,  0.1060],\n",
      "         [-0.4789,  0.0051],\n",
      "         [-0.4744,  0.1696],\n",
      "         [-0.5888, -0.0388]],\n",
      "\n",
      "        [[-0.5740,  0.2727],\n",
      "         [-0.8709,  0.1008],\n",
      "         [-0.8628,  0.1060],\n",
      "         [-0.4789,  0.0051],\n",
      "         [-0.4744,  0.1696],\n",
      "         [-0.5888, -0.0388]]], grad_fn=<UnsafeViewBackward0>)\n",
      "\n",
      " Queries \n",
      " tensor([[[-0.3536,  0.3965],\n",
      "         [-0.3021, -0.0289],\n",
      "         [-0.3015, -0.0232],\n",
      "         [-0.1353, -0.0978],\n",
      "         [-0.2052,  0.0870],\n",
      "         [-0.1542, -0.1499]],\n",
      "\n",
      "        [[-0.3536,  0.3965],\n",
      "         [-0.3021, -0.0289],\n",
      "         [-0.3015, -0.0232],\n",
      "         [-0.1353, -0.0978],\n",
      "         [-0.2052,  0.0870],\n",
      "         [-0.1542, -0.1499]]], grad_fn=<UnsafeViewBackward0>)\n",
      "\n",
      " Values \n",
      " tensor([[[-0.4519,  0.2216],\n",
      "         [-0.7142, -0.1961],\n",
      "         [-0.7127, -0.1971],\n",
      "         [-0.3809, -0.1557],\n",
      "         [-0.4861, -0.1597],\n",
      "         [-0.4213, -0.1501]],\n",
      "\n",
      "        [[-0.4519,  0.2216],\n",
      "         [-0.7142, -0.1961],\n",
      "         [-0.7127, -0.1971],\n",
      "         [-0.3809, -0.1557],\n",
      "         [-0.4861, -0.1597],\n",
      "         [-0.4213, -0.1501]]], grad_fn=<UnsafeViewBackward0>)\n",
      "Keys, Queries and Values - AFTER SPLITTING\n",
      "\n",
      " Keys: tensor([[[[-0.5740],\n",
      "          [ 0.2727]],\n",
      "\n",
      "         [[-0.8709],\n",
      "          [ 0.1008]],\n",
      "\n",
      "         [[-0.8628],\n",
      "          [ 0.1060]],\n",
      "\n",
      "         [[-0.4789],\n",
      "          [ 0.0051]],\n",
      "\n",
      "         [[-0.4744],\n",
      "          [ 0.1696]],\n",
      "\n",
      "         [[-0.5888],\n",
      "          [-0.0388]]],\n",
      "\n",
      "\n",
      "        [[[-0.5740],\n",
      "          [ 0.2727]],\n",
      "\n",
      "         [[-0.8709],\n",
      "          [ 0.1008]],\n",
      "\n",
      "         [[-0.8628],\n",
      "          [ 0.1060]],\n",
      "\n",
      "         [[-0.4789],\n",
      "          [ 0.0051]],\n",
      "\n",
      "         [[-0.4744],\n",
      "          [ 0.1696]],\n",
      "\n",
      "         [[-0.5888],\n",
      "          [-0.0388]]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      " Queries: tensor([[[[-0.3536],\n",
      "          [ 0.3965]],\n",
      "\n",
      "         [[-0.3021],\n",
      "          [-0.0289]],\n",
      "\n",
      "         [[-0.3015],\n",
      "          [-0.0232]],\n",
      "\n",
      "         [[-0.1353],\n",
      "          [-0.0978]],\n",
      "\n",
      "         [[-0.2052],\n",
      "          [ 0.0870]],\n",
      "\n",
      "         [[-0.1542],\n",
      "          [-0.1499]]],\n",
      "\n",
      "\n",
      "        [[[-0.3536],\n",
      "          [ 0.3965]],\n",
      "\n",
      "         [[-0.3021],\n",
      "          [-0.0289]],\n",
      "\n",
      "         [[-0.3015],\n",
      "          [-0.0232]],\n",
      "\n",
      "         [[-0.1353],\n",
      "          [-0.0978]],\n",
      "\n",
      "         [[-0.2052],\n",
      "          [ 0.0870]],\n",
      "\n",
      "         [[-0.1542],\n",
      "          [-0.1499]]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      " Values: tensor([[[[-0.4519],\n",
      "          [ 0.2216]],\n",
      "\n",
      "         [[-0.7142],\n",
      "          [-0.1961]],\n",
      "\n",
      "         [[-0.7127],\n",
      "          [-0.1971]],\n",
      "\n",
      "         [[-0.3809],\n",
      "          [-0.1557]],\n",
      "\n",
      "         [[-0.4861],\n",
      "          [-0.1597]],\n",
      "\n",
      "         [[-0.4213],\n",
      "          [-0.1501]]],\n",
      "\n",
      "\n",
      "        [[[-0.4519],\n",
      "          [ 0.2216]],\n",
      "\n",
      "         [[-0.7142],\n",
      "          [-0.1961]],\n",
      "\n",
      "         [[-0.7127],\n",
      "          [-0.1971]],\n",
      "\n",
      "         [[-0.3809],\n",
      "          [-0.1557]],\n",
      "\n",
      "         [[-0.4861],\n",
      "          [-0.1597]],\n",
      "\n",
      "         [[-0.4213],\n",
      "          [-0.1501]]]], grad_fn=<ViewBackward0>)\n",
      "Keys, Queries and Values - AFTER TRANSPOSE: (b, 'num_tokens', 'num_heads', head_dim) -> (b, 'num_heads', 'num_tokens', head_dim)\n",
      "\n",
      " Keys: tensor([[[[-0.5740],\n",
      "          [-0.8709],\n",
      "          [-0.8628],\n",
      "          [-0.4789],\n",
      "          [-0.4744],\n",
      "          [-0.5888]],\n",
      "\n",
      "         [[ 0.2727],\n",
      "          [ 0.1008],\n",
      "          [ 0.1060],\n",
      "          [ 0.0051],\n",
      "          [ 0.1696],\n",
      "          [-0.0388]]],\n",
      "\n",
      "\n",
      "        [[[-0.5740],\n",
      "          [-0.8709],\n",
      "          [-0.8628],\n",
      "          [-0.4789],\n",
      "          [-0.4744],\n",
      "          [-0.5888]],\n",
      "\n",
      "         [[ 0.2727],\n",
      "          [ 0.1008],\n",
      "          [ 0.1060],\n",
      "          [ 0.0051],\n",
      "          [ 0.1696],\n",
      "          [-0.0388]]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      " Queries: tensor([[[[-0.3536],\n",
      "          [-0.3021],\n",
      "          [-0.3015],\n",
      "          [-0.1353],\n",
      "          [-0.2052],\n",
      "          [-0.1542]],\n",
      "\n",
      "         [[ 0.3965],\n",
      "          [-0.0289],\n",
      "          [-0.0232],\n",
      "          [-0.0978],\n",
      "          [ 0.0870],\n",
      "          [-0.1499]]],\n",
      "\n",
      "\n",
      "        [[[-0.3536],\n",
      "          [-0.3021],\n",
      "          [-0.3015],\n",
      "          [-0.1353],\n",
      "          [-0.2052],\n",
      "          [-0.1542]],\n",
      "\n",
      "         [[ 0.3965],\n",
      "          [-0.0289],\n",
      "          [-0.0232],\n",
      "          [-0.0978],\n",
      "          [ 0.0870],\n",
      "          [-0.1499]]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      " Values: tensor([[[[-0.4519],\n",
      "          [-0.7142],\n",
      "          [-0.7127],\n",
      "          [-0.3809],\n",
      "          [-0.4861],\n",
      "          [-0.4213]],\n",
      "\n",
      "         [[ 0.2216],\n",
      "          [-0.1961],\n",
      "          [-0.1971],\n",
      "          [-0.1557],\n",
      "          [-0.1597],\n",
      "          [-0.1501]]],\n",
      "\n",
      "\n",
      "        [[[-0.4519],\n",
      "          [-0.7142],\n",
      "          [-0.7127],\n",
      "          [-0.3809],\n",
      "          [-0.4861],\n",
      "          [-0.4213]],\n",
      "\n",
      "         [[ 0.2216],\n",
      "          [-0.1961],\n",
      "          [-0.1971],\n",
      "          [-0.1557],\n",
      "          [-0.1597],\n",
      "          [-0.1501]]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      " attention scores: queries @ keys.transpose(2, 3) \n",
      " tensor([[[[ 2.0295e-01,  3.0793e-01,  3.0508e-01,  1.6934e-01,  1.6772e-01,\n",
      "            2.0820e-01],\n",
      "          [ 1.7340e-01,  2.6311e-01,  2.6067e-01,  1.4469e-01,  1.4330e-01,\n",
      "            1.7789e-01],\n",
      "          [ 1.7303e-01,  2.6255e-01,  2.6011e-01,  1.4438e-01,  1.4300e-01,\n",
      "            1.7751e-01],\n",
      "          [ 7.7677e-02,  1.1786e-01,  1.1677e-01,  6.4815e-02,  6.4194e-02,\n",
      "            7.9688e-02],\n",
      "          [ 1.1777e-01,  1.7869e-01,  1.7703e-01,  9.8267e-02,  9.7327e-02,\n",
      "            1.2082e-01],\n",
      "          [ 8.8499e-02,  1.3428e-01,  1.3304e-01,  7.3844e-02,  7.3138e-02,\n",
      "            9.0790e-02]],\n",
      "\n",
      "         [[ 1.0813e-01,  3.9973e-02,  4.2025e-02,  2.0194e-03,  6.7251e-02,\n",
      "           -1.5368e-02],\n",
      "          [-7.8833e-03, -2.9141e-03, -3.0637e-03, -1.4722e-04, -4.9028e-03,\n",
      "            1.1203e-03],\n",
      "          [-6.3209e-03, -2.3366e-03, -2.4565e-03, -1.1804e-04, -3.9311e-03,\n",
      "            8.9831e-04],\n",
      "          [-2.6661e-02, -9.8554e-03, -1.0361e-02, -4.9789e-04, -1.6581e-02,\n",
      "            3.7890e-03],\n",
      "          [ 2.3724e-02,  8.7696e-03,  9.2198e-03,  4.4304e-04,  1.4754e-02,\n",
      "           -3.3715e-03],\n",
      "          [-4.0878e-02, -1.5111e-02, -1.5886e-02, -7.6340e-04, -2.5423e-02,\n",
      "            5.8094e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0295e-01,  3.0793e-01,  3.0508e-01,  1.6934e-01,  1.6772e-01,\n",
      "            2.0820e-01],\n",
      "          [ 1.7340e-01,  2.6311e-01,  2.6067e-01,  1.4469e-01,  1.4330e-01,\n",
      "            1.7789e-01],\n",
      "          [ 1.7303e-01,  2.6255e-01,  2.6011e-01,  1.4438e-01,  1.4300e-01,\n",
      "            1.7751e-01],\n",
      "          [ 7.7677e-02,  1.1786e-01,  1.1677e-01,  6.4815e-02,  6.4194e-02,\n",
      "            7.9688e-02],\n",
      "          [ 1.1777e-01,  1.7869e-01,  1.7703e-01,  9.8267e-02,  9.7327e-02,\n",
      "            1.2082e-01],\n",
      "          [ 8.8499e-02,  1.3428e-01,  1.3304e-01,  7.3844e-02,  7.3138e-02,\n",
      "            9.0790e-02]],\n",
      "\n",
      "         [[ 1.0813e-01,  3.9973e-02,  4.2025e-02,  2.0194e-03,  6.7251e-02,\n",
      "           -1.5368e-02],\n",
      "          [-7.8833e-03, -2.9141e-03, -3.0637e-03, -1.4722e-04, -4.9028e-03,\n",
      "            1.1203e-03],\n",
      "          [-6.3209e-03, -2.3366e-03, -2.4565e-03, -1.1804e-04, -3.9311e-03,\n",
      "            8.9831e-04],\n",
      "          [-2.6661e-02, -9.8554e-03, -1.0361e-02, -4.9789e-04, -1.6581e-02,\n",
      "            3.7890e-03],\n",
      "          [ 2.3724e-02,  8.7696e-03,  9.2198e-03,  4.4304e-04,  1.4754e-02,\n",
      "           -3.3715e-03],\n",
      "          [-4.0878e-02, -1.5111e-02, -1.5886e-02, -7.6340e-04, -2.5423e-02,\n",
      "            5.8094e-03]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "\n",
      " attention scores after masking \n",
      " tensor([[[[ 0.2029,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.1734,  0.2631,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.1730,  0.2625,  0.2601,    -inf,    -inf,    -inf],\n",
      "          [ 0.0777,  0.1179,  0.1168,  0.0648,    -inf,    -inf],\n",
      "          [ 0.1178,  0.1787,  0.1770,  0.0983,  0.0973,    -inf],\n",
      "          [ 0.0885,  0.1343,  0.1330,  0.0738,  0.0731,  0.0908]],\n",
      "\n",
      "         [[ 0.1081,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0079, -0.0029,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0063, -0.0023, -0.0025,    -inf,    -inf,    -inf],\n",
      "          [-0.0267, -0.0099, -0.0104, -0.0005,    -inf,    -inf],\n",
      "          [ 0.0237,  0.0088,  0.0092,  0.0004,  0.0148,    -inf],\n",
      "          [-0.0409, -0.0151, -0.0159, -0.0008, -0.0254,  0.0058]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2029,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.1734,  0.2631,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.1730,  0.2625,  0.2601,    -inf,    -inf,    -inf],\n",
      "          [ 0.0777,  0.1179,  0.1168,  0.0648,    -inf,    -inf],\n",
      "          [ 0.1178,  0.1787,  0.1770,  0.0983,  0.0973,    -inf],\n",
      "          [ 0.0885,  0.1343,  0.1330,  0.0738,  0.0731,  0.0908]],\n",
      "\n",
      "         [[ 0.1081,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0079, -0.0029,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0063, -0.0023, -0.0025,    -inf,    -inf,    -inf],\n",
      "          [-0.0267, -0.0099, -0.0104, -0.0005,    -inf,    -inf],\n",
      "          [ 0.0237,  0.0088,  0.0092,  0.0004,  0.0148,    -inf],\n",
      "          [-0.0409, -0.0151, -0.0159, -0.0008, -0.0254,  0.0058]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "\n",
      " attention weights \n",
      " tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4776, 0.5224, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3140, 0.3434, 0.3426, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2458, 0.2559, 0.2556, 0.2427, 0.0000, 0.0000],\n",
      "          [0.1967, 0.2090, 0.2087, 0.1929, 0.1927, 0.0000],\n",
      "          [0.1649, 0.1726, 0.1724, 0.1625, 0.1624, 0.1653]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4988, 0.5012, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3325, 0.3338, 0.3337, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2463, 0.2505, 0.2504, 0.2528, 0.0000, 0.0000],\n",
      "          [0.2025, 0.1995, 0.1996, 0.1978, 0.2007, 0.0000],\n",
      "          [0.1625, 0.1667, 0.1666, 0.1691, 0.1650, 0.1702]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4776, 0.5224, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3140, 0.3434, 0.3426, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2458, 0.2559, 0.2556, 0.2427, 0.0000, 0.0000],\n",
      "          [0.1967, 0.2090, 0.2087, 0.1929, 0.1927, 0.0000],\n",
      "          [0.1649, 0.1726, 0.1724, 0.1625, 0.1624, 0.1653]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4988, 0.5012, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3325, 0.3338, 0.3337, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2463, 0.2505, 0.2504, 0.2528, 0.0000, 0.0000],\n",
      "          [0.2025, 0.1995, 0.1996, 0.1978, 0.2007, 0.0000],\n",
      "          [0.1625, 0.1667, 0.1666, 0.1691, 0.1650, 0.1702]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      " attention weights after dropout\n",
      " tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4776, 0.5224, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3140, 0.3434, 0.3426, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2458, 0.2559, 0.2556, 0.2427, 0.0000, 0.0000],\n",
      "          [0.1967, 0.2090, 0.2087, 0.1929, 0.1927, 0.0000],\n",
      "          [0.1649, 0.1726, 0.1724, 0.1625, 0.1624, 0.1653]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4988, 0.5012, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3325, 0.3338, 0.3337, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2463, 0.2505, 0.2504, 0.2528, 0.0000, 0.0000],\n",
      "          [0.2025, 0.1995, 0.1996, 0.1978, 0.2007, 0.0000],\n",
      "          [0.1625, 0.1667, 0.1666, 0.1691, 0.1650, 0.1702]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4776, 0.5224, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3140, 0.3434, 0.3426, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2458, 0.2559, 0.2556, 0.2427, 0.0000, 0.0000],\n",
      "          [0.1967, 0.2090, 0.2087, 0.1929, 0.1927, 0.0000],\n",
      "          [0.1649, 0.1726, 0.1724, 0.1625, 0.1624, 0.1653]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4988, 0.5012, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3325, 0.3338, 0.3337, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2463, 0.2505, 0.2504, 0.2528, 0.0000, 0.0000],\n",
      "          [0.2025, 0.1995, 0.1996, 0.1978, 0.2007, 0.0000],\n",
      "          [0.1625, 0.1667, 0.1666, 0.1691, 0.1650, 0.1702]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      " context vector : attention weights * values and then transpose(1,2) \n",
      " tensor([[[[-0.4519],\n",
      "          [ 0.2216]],\n",
      "\n",
      "         [[-0.5889],\n",
      "          [ 0.0122]],\n",
      "\n",
      "         [[-0.6313],\n",
      "          [-0.0576]],\n",
      "\n",
      "         [[-0.5685],\n",
      "          [-0.0832]],\n",
      "\n",
      "         [[-0.5541],\n",
      "          [-0.0964]],\n",
      "\n",
      "         [[-0.5311],\n",
      "          [-0.1077]]],\n",
      "\n",
      "\n",
      "        [[[-0.4519],\n",
      "          [ 0.2216]],\n",
      "\n",
      "         [[-0.5889],\n",
      "          [ 0.0122]],\n",
      "\n",
      "         [[-0.6313],\n",
      "          [-0.0576]],\n",
      "\n",
      "         [[-0.5685],\n",
      "          [-0.0832]],\n",
      "\n",
      "         [[-0.5541],\n",
      "          [-0.0964]],\n",
      "\n",
      "         [[-0.5311],\n",
      "          [-0.1077]]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      " context vector after combining all the heads \n",
      " tensor([[[-0.4519,  0.2216],\n",
      "         [-0.5889,  0.0122],\n",
      "         [-0.6313, -0.0576],\n",
      "         [-0.5685, -0.0832],\n",
      "         [-0.5541, -0.0964],\n",
      "         [-0.5311, -0.1077]],\n",
      "\n",
      "        [[-0.4519,  0.2216],\n",
      "         [-0.5889,  0.0122],\n",
      "         [-0.6313, -0.0576],\n",
      "         [-0.5685, -0.0832],\n",
      "         [-0.5541, -0.0964],\n",
      "         [-0.5311, -0.1077]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      " context vector projection \n",
      " tensor([[[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]],\n",
      "\n",
      "        [[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "\n",
      " Output: Context Vector \n",
      "\n",
      "tensor([[[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]],\n",
      "\n",
      "        [[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      " context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "        print(\"\\n Init: \\n\", self)\n",
    "        print(\"Query weight: \\n\", self.W_query.weight)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"[forward]: input tensor \\n\", x)\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        print(\"\\n input shape (b, num_tokesn,d_in): \", x.shape)\n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        print(\"Keys, Queries and Values - BEFORE SPLITTING \\n\")\n",
    "        print(\"\\n Keys \\n\", keys)\n",
    "        print(\"\\n Queries \\n\", queries)\n",
    "        print(\"\\n Values \\n\", values)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        \n",
    "        print(\"Keys, Queries and Values - AFTER SPLITTING\")\n",
    "        print(\"\\n Keys:\", keys)\n",
    "        print(\"\\n Queries:\", queries)\n",
    "        print(\"\\n Values:\", values)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        \n",
    "        print(\"Keys, Queries and Values - AFTER TRANSPOSE: (b, 'num_tokens', 'num_heads', head_dim) -> (b, 'num_heads', 'num_tokens', head_dim)\")\n",
    "        print(\"\\n Keys:\", keys)\n",
    "        print(\"\\n Queries:\", queries)\n",
    "        print(\"\\n Values:\", values)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "        print(\"\\n attention scores: queries @ keys.transpose(2, 3) \\n\", attn_scores)\n",
    "        \n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        print(\"\\n attention scores after masking \\n\", attn_scores)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        print(\"\\n attention weights \\n\", attn_weights)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        print(\"\\n attention weights after dropout\\n\", attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "        print(\"\\n context vector : attention weights * values and then transpose(1,2) \\n\", context_vec) \n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        print(\"\\n context vector after combining all the heads \\n\", context_vec)\n",
    "        \n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "        print(\"\\n context vector projection \\n\", context_vec)\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 2\n",
    "\n",
    "print(\"\\n batch:\", batch)\n",
    "print(\"\\n batch shape:\", batch.shape)\n",
    "print(\"\\n batch size:\", batch_size)\n",
    "print(\"\\n context_length:\", context_length)\n",
    "print(\"\\n d_in:\", d_in)\n",
    "print(\"\\n d_out:\", d_out)\n",
    "print(f\"\\n Calling MultiHeadAttention(d_in:{d_in}, d_out:{d_out}, context_length:{context_length}, dropout:0.0, num_heads=2)\")\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(\"\\n\\n Output: Context Vector \\n\")\n",
    "print(context_vecs)\n",
    "print(\"\\n context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that the above is essentially a rewritten version of `MultiHeadAttentionWrapper` that is more efficient\n",
    "- The resulting output looks a bit different since the random weight initializations differ, but both are fully functional implementations that can be used in the GPT class we will implement in the upcoming chapters\n",
    "- Note that in addition, we added a linear projection layer (`self.out_proj`) to the `MultiHeadAttention` class above. This is simply a linear transformation that doesn't change the dimensions. It's a standard convention to use such a projection layer in LLM implementation, but it's not strictly necessary (recent research has shown that it can be removed without affecting the modeling performance; see the further reading section at the end of this chapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- On a big-picture level, in the previous `MultiHeadAttentionWrapper`, we stacked multiple single-head attention layers that we combined into a multihead attention layer. The `MultiHeadAttention` class takes an integrated approach. It starts with a multi-head layer and then internally splits this layer into individual attention heads.\n",
    "\n",
    "- In the `MultiheadAttentionWrapper` class with two attention heads, we initialized two weight matrices Wq1 and Wq2 and computed two query matrices Q1 and Q2 as illustrated here. \n",
    "\n",
    "    <img src=\"./images/12_multihead_wrapper_split.png\" width=\"400px\">\n",
    "\n",
    "- In the `MultiheadAttention` class, we initialize one larger weight matrix Wq , only perform one matrix multiplication with the inputs to obtain a query matrix Q, and then split the query matrix into Q1 and Q2 as shown in this figure. We do the same for the keys and values, which are not shown to reduce visual clutter.\n",
    "    - The splitting of the query, key, and value tensors, as depicted  here, is achieved through tensor reshaping and transposing operations using PyTorch's `.view` and `.transpose` methods. The input is first transformed (via linear layers for queries, keys, and values) and then reshaped to represent multiple heads.\n",
    "    \n",
    "    - The key operation is to split the `d_out` dimension into `num_heads` and `head_dim`, where `head_dim = d_out / num_heads`. This splitting is then achieved using the `.view` method: a tensor of dimensions `(b, num_tokens,d_out)` is reshaped to dimension `(b,num_tokens, num_heads, head_dim)`.\n",
    "\n",
    "    <img src=\"./images/12_multihead_attn_class.png\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[[[0.2745, 0.6584, 0.2775, 0.8573], #A\n",
    "                    [0.8993, 0.0390, 0.9268, 0.7388],\n",
    "                    [0.7179, 0.7058, 0.9156, 0.4340]],\n",
    "                    [[0.0772, 0.3565, 0.1479, 0.5331],\n",
    "                    [0.4066, 0.2318, 0.4545, 0.9737],\n",
    "                    [0.4606, 0.5159, 0.4220, 0.5786]]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1.3208, 1.1631, 1.2879],\n",
      "          [1.1631, 2.2150, 1.8424],\n",
      "          [1.2879, 1.8424, 2.0402]],\n",
      "\n",
      "         [[0.4391, 0.7003, 0.5903],\n",
      "          [0.7003, 1.3737, 1.0620],\n",
      "          [0.5903, 1.0620, 0.9912]]]])\n"
     ]
    }
   ],
   "source": [
    "print(a @ a.transpose(2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First head:\n",
      " tensor([[1.3208, 1.1631, 1.2879],\n",
      "        [1.1631, 2.2150, 1.8424],\n",
      "        [1.2879, 1.8424, 2.0402]])\n",
      "\n",
      "Second head:\n",
      " tensor([[0.4391, 0.7003, 0.5903],\n",
      "        [0.7003, 1.3737, 1.0620],\n",
      "        [0.5903, 1.0620, 0.9912]])\n"
     ]
    }
   ],
   "source": [
    "first_head = a[0, 0, :, :]\n",
    "first_res = first_head @ first_head.T\n",
    "print(\"First head:\\n\", first_res)\n",
    "\n",
    "second_head = a[0, 1, :, :]\n",
    "second_res = second_head @ second_head.T\n",
    "print(\"\\nSecond head:\\n\", second_res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
